---
title: Transformers from first principles
date: 2023-10-06
tags:
  - seed
enableToc: false
---

In this page, we will implement [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf), from scratch. This is a very simple implementation and is not optimized for speed. The goal is to understand the architecture of the transformer. If you are a basic understanding of linear algrebra and python, you should be able to follow along.

Subcomponents of the transformer which need to be coded out in seperate sections:

## Architecture
* [[notes/layerexplain|Layer]]
    * [[notes/layerexplain#Kaiming initialization |Kaiming Initialization]]
* Feed Forward network 
* Attention Block
* Self Attention Block
* Multi Head Attention Block
* Embedding Layer
* Positional Encoding
* Layer normalization
* Batch normalization
* Activation functions
* Masking
* Dropout
* Encoder
* Decoder
* Complete transformer

## Training
* Optimizers
* Loss functions
* Regularization
* Weight decay
* Learning rate schedulers

## Decoding
* Beam search
* Evaluation metrics



* Layer Norm 
* Attention
* Self Attention
* Multi Head attention 
* Positional Encoding
* Dropout 
* Batch normalization
* Activation functions
* Optimizers
* Loss functions
* Encoders
* Decoders
* Embedding Layer
* Positional Encoding
* Regularization
* Decoding 
* Tokenizers
* Weight decay
* Learning rate schedulers
* Masking
* Beam search
* Evaluation metrics

{"gallery":{"title":"Gallery","links":[],"tags":[],"content":"Collection of my favorite pictures\n\n    \n\n\nOng-Ard Satrabhandhu is a Thai architect whose work is known for its masterful combination of classical principles and traditional Thai qualities\n\n\n    \n\n\nPuppies, by Maruyama Ōkyo, ca. 1790\n\n\n    \n\n\nElephant Family, by Marc Allante\n\n\n    \n\n\nUnconquered sol on Twitter\n\n\n    \n\n\nGhost in the shell Anime\n\n\n    \n\n\nKorean Buddhist Library\n\n\n    \n\n\nBuffalo Cops in Thailand\n\n\n    \n    \n\nPrometheus in chains by Frantisek Kupka\n\n\n    \n\n\nOwl annoyed by a squirrel\n\n\n    \n\n\nRadha Krishna\n\n\n    \n\n\nMoon like Saturn\n\n\n    \n\n\nGodess Saraswati\n\n\n    \n\n\nThe Ruins of Pompeii art print by Filippo Palizzi\n\n\n    \n\n\nHouse in the hills\n\n\n    \n\n\nReplicant and an Angel\n\n\n    \n\n\nBodhidharma by Yoshitoshi\n\n\n    \n\n\nEnryaku-ji Temple, Kyoto\n\n\n    \n\n\nCute band of Monkeys playing in the snow\n\n\n    \n\n\nRiverside Church during my studies at Columbia University\n\n\n    \n\n\nTomb of Tutankhamun\n\n\n    \n\n\nOil rig in the North Sea\n\n\n    \n\n\nCute pigeon painting\n\n\n    \n\n\nSpace Shuttle and F-16\n\n\n    \n\n\nView of a Nebula from a space-ship window\n\n\n    \n\n\nNeat networking in a Data Center\n\n\n    \n\n\nRajput Horse\n\n\n    \n\n\nConcordian eclipse\n\n\n    \n\n\nTaj Lake Palace Hotel, Udaipur\n\n\n    \n\n\nMinoan Women by Bakst\n\n\n    \n\n\nOil from Blade Runner\n"},"index":{"title":"Eternal Horizons","links":["gallery","lighl16","notes","tags/dreams","tags/seed","tags/sapling","tags/evergreen"],"tags":[],"content":"\nA fault in the space-time continuum where two normally distant points of space touch one another\n\n\n    \n\nWelcome to my blog, I will mainly talk about the NLP, Complexity theory, re-engineering products, modern-day Hindutva’s deontological ethics, lack of meaning in a world approaching AGI and computational search problems.\nEmbark on this voyage with me as we seek to discover our unique silhouettes in a world that often feels devoid of true significance. Hope you leave my blog with a sense of meaning and a little bit of inspiration.\nYou can browse on the left, explore the graph on the right, or search the whole site with ⌘+K.\nMeanwhile, take a look at my gallery of my favorite pictures and my project for the month\nAll notes are in one of four states:\n\nDreams: messages from the deep\nSeed: brief ideas\nSapling: fully-formed thoughts\nEvergreen: permanent, constantly growing\n\nThank you Ishan and Wayne for the template"},"lighl16":{"title":"Light L16 Re-engineering","links":["light/lri","light/ransac","light/sift","light/bayer","light/vignetting","light/tof","light/10bit"],"tags":["evergreen"],"content":"In early 2018, Light.co released a groundbreaking camera. It was claimed to be engineering marvel. It takes 16 different images and using computational photogrammetry combines the 16 different images into one. Unfortunately the cameras software was badly executed. In this blog post, we will think of ways to re-engineer the device and try to recreate the software using the same methods.\nAll this work, wouldn’t be possible without gennyble and all the members in light.co discord server.\nHow it works under the hood ? §\nL16 simulates all the focal lengths in between 28mm, 70mm, and 150mm by combining data from multiple camera modules. So instead of digitally zooming in on the 28mm image to make it look like it was shot at 40mm, it’s replicating that focal length by stitching images together on the fly.\n\n    \n\nSince the company was acquired by John Deere, all the support for the device is non existant. We will have to understand all the basic parts of all the software by re-engineering every aspect of it.\nWe have mainly three parts which we will have to figure out. These include:\n\nLumen Software used for merging\nRaw file generated .LRI\nCamera APK and C++ Files\n\nSince, this is a very engineering heavy project and relies on a lot of people. We have the following subpages, to better understand terms and files. Please click on each link to view.\n\nRanSAC\nFoward Matrix and Color Matrix\nIlluminant type\nDe-Mosaic\nSIFT\nProtoBuff\nGamma\nBayer Jpeg\nVignetting characterization\nToFCalibration\nTiFF files\n10 Bit Images\n\nDetailed Steps to be completed §\nRaw file creation §\nThe current process deals with .lri format. In the future, each individual image needs to be converted to .RAW format image which can be editied\nStiching §\nThe whole stiching pipeline involves a lot of stages these are:\nEnhance Feature Matching: §\n\nPreprocess images to enhance features: Applying filters like edge enhancement can make feature points more distinct.\nTweak feature detection parameters: If using custom stitching algorithms (beyond OpenCV’s built-in Stitcher), adjusting parameters of feature detectors (e.g., SIFT, SURF) may yield better matches.\n\nImprove Alignment: §\n\nManual alignment: For a small number of images, manually selecting matching points can improve the homography estimation.\nRANSAC for Homography Estimation: Ensure the RANSAC (Random Sample Consensus) algorithm is used to find the best homography matrix that aligns images. It’s robust against outliers.\n\nSeam Correction: §\n\nSeam finding algorithms: Utilize seam finding and blending algorithms to minimize visible seams. OpenCV offers functions like createMultiBandBlender() which might produce better results than the default blending options in the Stitcher class.\nManual editing: In cases where automated solutions fail, manual editing in photo editing software might be necessary to correct mismatches or blend seams.\n\nExposure and Color Correction: §\n\nAdjust exposure and color balance: Before stitching, adjust the images to match in terms of exposure and color balance to reduce visual discrepancies.\nApply gain compensation: Some stitching libraries offer gain compensation to equalize the brightness and color across stitched images.\n\nAdvanced Techniques and research: §\n\nHomography refinement: After an initial stitch, refine the homography matrices by minimizing the discrepancy between the overlapping areas.\nMulti-scale stitching: Perform stitching at multiple scales or resolutions to ensure both large structures and fine details are well aligned.\n"},"light/10bit":{"title":"10Bit","links":[],"tags":[],"content":"A 10-bit image refers to the bit depth of an image, which is a measure of the number of bits used to represent the color of a single pixel. In a 10-bit image, each color channel of a pixel is represented with 10 bits of data. This means that for a standard RGB (Red, Green, Blue) color model, each channel can display 2^10 or 1024 different shades of intensity.\nCompared to the more common 8-bit images, which can display 256 shades per channel, 10-bit images can display a much wider range of colors. This results in smoother gradients and more detailed color representation, reducing the risk of banding in images.\nHere’s a quick comparison in terms of potential colors:\n\n8-bit per channel: 256 levels per channel, resulting in 16.7 million possible colors (256^3).\n10-bit per channel: 1024 levels per channel, resulting in over 1 billion possible colors (1024^3).\n\nThis increased color depth is particularly beneficial in professional photo and video editing, where it allows for more precise adjustments without losing detail or causing posterization."},"light/bayer":{"title":"Bayer JPEG","links":[],"tags":[],"content":"All of these investigations are from @gennyble, Please go say hi!\nThe BayerJPEG is a strange format used by the Light L16… sometimes. We don’t yet know when it switches from it’s normal packed 10-bit raw format to this, or why.\nBayerJPEG Header §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsizetypemeaning4 bytesStringMagic Number “BJPG”4 bytesu32Format type  0: colour  1: for monochrome4 bytesu32Length of Jpeg 04 bytesu32Length of Jpeg 14 bytesu32Length of Jpeg 24 bytesu32Length of Jpeg 31552 bytesunknown\nForamt Type: Monochrome\nJpeg0 contains a full resolution grayscale image\nFormat Type: Colour\nThe bayered image is split across the four Jpeg, one\nfor each colour location.\nI.E. an image from the ar1335 sensor, color filter bggr, you’d get\n\n1 jpeg for the blue channel\n2 jpeg for each green location\n1 jpeg for the red channel\n\nIt’s not currently known if these are in the order you’d expect.\nConsiderations\nWhen the L16 decides to use BayerJPEG, it has to save four copies of each frame. A JPEG is limited to a bit depth of eight, but the sensors output 10-bit data. In order to not loose 75% of the precision, they seemingly divide the image into fours and expect you to sum them later."},"light/demosaic":{"title":"demosaic","links":[],"tags":[],"content":""},"light/forward":{"title":"forward","links":[],"tags":[],"content":""},"light/gamma":{"title":"gamma","links":[],"tags":[],"content":""},"light/lri":{"title":"LRI File","links":["light/bayer"],"tags":[],"content":"All of these investigations are from @gennyble, Please go say hi!\nAnatomy of an LRI §\nThe file is made up of many blocks, usually 10 or 11 but cases of 40 have occurred.\nBlocks start with a header and contain some data. There is always a protobuf message within that data, and sometimes stuff like the images themselves.\nBlock Header §\nThe header is 32 bytes long, uses little-endian, and goes as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbytestypemeaning4-Signature: “LELR”8u64block length8u64message offset from block start4u32message length1u8message type, see below7-reserved\nmessage to mean the protobuf message\nMessage Type\n0: LightHeader ([proto][lh-proto])\n1: ViewPreferences ([proto][vp-proto])\n2: GPSData ([proto][gps-proto])\n\nGo through all the blocks for all the details as the information might be split across multiple branches\nhighlighting key parts §\nblocks have messages and that’s pretty much all you need to know. you can look at the protobuf definitions and pretty readily build a parser, but there are some things i’d like to mention, too.\nLightHeader §\nThe most important header and frustratingly fractured between multiple blocks.\nRAW Images §\nWhat we’re all here for, maybe.\nzero or more CameraModule  are collected in the modules. In a CameraModule we see a sensor_data_surface of type Surface (see line 33 of the CameraModule proto).\n\n\nstart might indicate a crop, but has always been (0,0) in my experience.\nsize gives the width/height of the image.\ndata_offset is the start of the image from the beginning of the block (meaning: it includes the length of the header).\nformat indicates how we’re meant to interpret the image data. It can be a few different things, but i’ve only seen RAW_BAYER_JPEG and RAW_PACKED_10BPP.\nrow_stride gives you the number of bytes per row the image takes up. Multiply this by the width to get the size of the image (except Bayer JPEG; see below)\n\nLet’s talk about Bayer JPEG. §\nWe don’t currently understand why the L16 makes these, just that it does. If it’s from a colour sensor, you’ll get four half-res JPEG (one for each bayer position). If it’s monochrome, you’ll get one full-res JPEG. For more information go here: Bayer Jpeg.\nIn either colour-case, the row_stride in the sensor_data_surface will be 0. You’ll have to parse the Bayer JPEG header to get the length of the sensor’s image data.\nthat’s enough BayerJPEG §\nGoing back to CameraModule, there’s some more important data for image interpretation. You’ll want the id which indicates which camera took the exposure. We can map this to a sensor model later! Grab sensor_bayer_red_override while you’re at it. It’ll help with figuring out what CFA we need to use for debayering.\nBack in the LightHeader now we’ll go to hw_info, type HwInfo , then to camera which is a CameraModuleHwInfo (described on line 8 in the HwInfo definition). From this we can associate a CameraID, id, with a SensorType, sensor. It might be good to note here that there are quite a few SensorType defined, but i’ve only ever seen AR1335 and it’s monochrome variant. Perhaps the others were used in development?\n\n\n\tnote on the above\n\t\n\t\tI&#039;m not sure how necessary it is to make this map. Is it ridiculous to assume that the CameraID are consistent between L16 and they they are the same SensorType? This could very well be hard coded with very little harm.\n\t\n\nsensor_bayer_red_override §\nAs far as I can tell this tells us how to shift the CFA for the specific camera. I don’t know why it’s different; perhaps it’s cropped before writing to disk?\nAnyway, the x/y you’re given seem to map to where Red should be in the 2x2 array. For example, if you have a BGGR cfa and your override is x=1 y=0, you should end up with GRBG (See the ascii diagram below).\nBGGR cfa                 GRBG cfa\n\nB G B G                  G R G R\nG R G R  -&gt; override -&gt;  B G B G\nB G B G     x:1, y:0     G R G R\nG R G R                  G B G B\n\nColour Data §"},"light/lumen":{"title":"lumen","links":[],"tags":[],"content":""},"light/protobuff":{"title":"protobuff","links":[],"tags":[],"content":""},"light/ransac":{"title":"Ransac","links":[],"tags":[],"content":"The RANSAC (Random Sample Consensus) algorithm is a robust estimation method used to fit a mathematical model to a dataset that contains outliers. It is widely used in computer vision for tasks such as estimating the fundamental matrix in stereo vision, finding homographies between image pairs in panorama stitching, and fitting geometric shapes to sets of points. RANSAC works by repeatedly selecting a random subset of the original data to estimate model parameters, then validating this model against the entire dataset to find the best-fitting model that has the highest number of inliers.\nThe general steps of RANSAC are :\n\nSelect a Random Subset: Randomly select a minimal subset of points necessary to estimate the model parameters.\nEstimate the Model: Estimate the model parameters using this subset.\nIdentify Inliers: Determine which points from the entire dataset fit this model within a certain threshold (these points are considered inliers).\nRepeat: Repeat steps 1-3 for a predefined number of iterations, each time updating the best-fitting model if the current iteration has more inliers.\nBest Model: After all iterations, choose the model with the highest number of inliers.\nRANSAC is particularly useful because it is capable of providing a good estimation even when the data contains a significant number of outliers.\n\nExample in OpenCV: Estimating a Homography with RANSAC §\nBelow is an example of using RANSAC in OpenCV to estimate a homography between two images. This is commonly used in applications like image stitching or when you want to understand the transformation between two scenes.\nimport cv2\nimport numpy as np\n\n# Load your images\nimage1 = cv2.imread(&#039;image1.jpg&#039;)\nimage2 = cv2.imread(&#039;image2.jpg&#039;)\n\n# Convert images to grayscale\ngray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\ngray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n\n# Initialize SIFT detector\nsift = cv2.SIFT_create()\n\n# Find keypoints and descriptors with SIFT\nkeypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\nkeypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n\n# Create a BFMatcher object\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# Match descriptors\nmatches = bf.match(descriptors1, descriptors2)\n\n# Sort them in the order of their distance\nmatches = sorted(matches, key=lambda x:x.distance)\n\n# Draw first 10 matches\nimg_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\n# Extract location of good matches\npoints1 = np.zeros((len(matches), 2), dtype=np.float32)\npoints2 = np.zeros_like(points1)\n\nfor i, match in enumerate(matches):\n    points1[i, :] = keypoints1[match.queryIdx].pt\n    points2[i, :] = keypoints2[match.trainIdx].pt\n\n# Find Homography\nH, status = cv2.findHomography(points1, points2, cv2.RANSAC)\n\n# Use the Homography Matrix to warp the images\nresult = cv2.warpPerspective(image1, H, (image1.shape[1] + image2.shape[1], image1.shape[0]))\nresult[0:image2.shape[0], 0:image2.shape[1]] = image2\n\n# Display the stitched image\ncv2.imshow(&#039;Result&#039;, result)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\nThis code performs the following steps:\n\nLoads two images and converts them to grayscale.\nDetects SIFT keypoints and computes descriptors for both images.\nUses a Brute-Force Matcher to match the descriptors between the two images.\nUses RANSAC to estimate a homography matrix that best aligns the two images.\nWarps one image onto the other using the estimated homography to demonstrate how they align.\n"},"light/sift":{"title":"SIFT","links":[],"tags":[],"content":"The SIFT (Scale-Invariant Feature Transform) algorithm in OpenCV is a feature detection algorithm used in computer vision tasks to detect and describe local features in images. It’s useful for tasks like object recognition, image stitching, and tracking because it’s invariant to scale, rotation, and partially invariant to change in illumination and 3D viewpoint.\nHere’s a simple example of how to use the SIFT algorithm in OpenCV to detect keypoints in an image:\npip install opencv-python opencv-contrib-python\n\nimport cv2\nimport numpy as np\n\n# Load the image\nimage = cv2.imread(&#039;path_to_your_image.jpg&#039;)\n\n# Convert the image to grayscale\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# Initialize the SIFT detector\nsift = cv2.SIFT_create()\n\n# Detect keypoints in the image\nkeypoints = sift.detect(gray, None)\n\n# Draw the keypoints on the image\nkeypoint_image = np.zeros_like(image)\nkeypoint_image = cv2.drawKeypoints(image, keypoints, keypoint_image, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\n# Display the image with keypoints\ncv2.imshow(&#039;SIFT Keypoints&#039;, keypoint_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\nIn this code:\n\nWe load an image and convert it to grayscale because SIFT operates on single-channel images.\nWe initialize the SIFT feature detector using cv2.SIFT_create().\nWe detect keypoints in the grayscale image using the detect method.\nWe draw these keypoints on the original image using cv2.drawKeypoints for visualization.\nFinally, we display the image with keypoints.\n"},"light/tof":{"title":"ToF Calibration","links":[],"tags":[],"content":"Time-of-Flight Calibration, relates to the calibration process for Time-of-Flight (ToF) cameras or sensors. ToF technology measures the time it takes for light to travel from the sensor to the subject and back, allowing the device to accurately map the distance or depth of objects in its field of view. This technology is used in various applications, including 3D imaging, augmented reality (AR), robotics, and security systems.\nToF Calibration is crucial for ensuring that the depth information captured by the ToF sensor is accurate. Here’s what the calibration process typically involves:\nDistance Calibration §\nThis involves adjusting the sensor’s measurements so that they accurately reflect the true distances. Due to various factors such as temperature, the speed of light through different media, or manufacturing variances, the raw measurements from a ToF sensor may not perfectly match the actual distances. Calibration ensures that the sensor’s output corresponds accurately to the real-world distances.\nGeometric Calibration §\nSince ToF sensors are often used in conjunction with other imaging sensors (e.g., RGB cameras), geometric calibration is necessary to align the depth data with data from other sensors. This ensures that the depth information correctly matches up with the corresponding visual information, which is essential for applications like augmented reality, where virtual objects must accurately overlay on the real world.\nReflectivity Calibration §\nReflectivity or intensity calibration accounts for how different surfaces reflect light differently, affecting the sensor’s ability to measure distance accurately. Some surfaces might absorb more light, while others reflect it more, leading to variations in the measured times that do not correspond to actual distance differences. Calibration processes adjust for these variations to ensure consistent depth measurements across different types of surfaces.\nError Correction §\nCalibration also involves identifying and correcting for systematic errors in the sensor’s measurements, such as those caused by the sensor’s internal components or external environmental factors.\nCalibrating a ToF sensor can involve using calibration targets at known distances and positions, environmental controls to minimize external factors, and sophisticated software algorithms to process the calibration data. Proper calibration is essential for the accurate functioning of ToF-based systems, especially in applications requiring high precision in distance and depth measurements."},"light/vignetting":{"title":"Vignetting","links":[],"tags":[],"content":"Vignetting characterization refers to the process of identifying and measuring vignetting, which is a reduction in image brightness or saturation at the periphery compared to the image center. Vignetting is a common optical phenomenon that can occur for various reasons, such as the physical properties of lens elements, lens hoods, or filters that block some light from reaching the outer portions of the image sensor. It can also be caused by the natural falloff of light from the center to the edge of the image field due to the angle at which light rays enter the lens elements.\nIn the context of camera calibration and image processing, vignetting characterization is crucial for several reasons:\n\n\nCorrection in Post-Processing: By understanding the vignetting characteristics of a specific camera-lens setup, software can correct for this effect, leading to more uniformly exposed images. This is especially important in professional photography and high-precision applications like photogrammetry, where image consistency and accuracy are crucial.\n\n\nEnhanced Image Quality: Correcting vignetting can improve the overall aesthetic and technical quality of images, making them more appealing and accurate representations of the scene.\n\n\nLens and Camera Evaluation: Vignetting characterization can be used to evaluate the performance of camera lenses under different conditions, such as various aperture settings, focal lengths, and focus distances. This information can be valuable for lens manufacturers during development and for photographers when selecting equipment for specific use cases.\n\n\nThe process typically involves capturing images of uniform, brightly lit surfaces or specialized test charts at different settings, then analyzing these images to quantify the vignetting effect. This analysis can be done using various image processing software tools. The characterization might result in a vignetting profile that describes how the vignetting effect varies across the image field and under different shooting conditions. This profile can then be used to automatically correct images taken under similar conditions, significantly improving the image quality or preparing images for further analysis or processing."},"notes/Books":{"title":"Sci-Fi Books","links":[],"tags":["evergreen"],"content":""},"notes/Idea-List":{"title":"Idea List","links":[],"tags":["dreams"],"content":"Open ideas, if you would want to collaborate on any of these, please let me know.\n\nLight L16 stiching algorithm\n\nReplace the method of laplacian blending with a more advanced method. Specifically try Alpha blending and cyclendrical blending\n\n\nAutomatic Minecraft Player and Crafter\n\nShow videos of builds and recreate them in your world\n\n\nNvidia Omniverse 3D asset creation\n\nCreate a 3D assets and place it in a 3d Location\nProgramatic world generation\nSora doesn’t follow geometries, omniverse can help fix\n\n\nObjaverse Search\n\n3D asset search is broken. It doesn’t have any open source ranking algorithm.\n\n\nRAG at scale\n\nProblem: RAG doesn’t scale well. The embeddings overlap and the search return non-relevant results\n\n\n"},"notes/Internet-Gems":{"title":"Internet Gems","links":[],"tags":["evergreen"],"content":"PowerPoint Programming: https://youtu.be/_3loq22TxSc?si=2fth674yzFu0Wa-X\nProject Greyball: https://www.nytimes.com/2017/03/03/technology/uber-greyball-program-evade-authorities.html\nAGI Futures: https://roonscape.ai/p/agi-futures"},"notes/Library":{"title":"Library","links":["notes/Podcasts","Cool-Tools","notes/Textbooks","Courses"],"tags":["evergreen"],"content":"Have a nice read, hope you find something you like :)\nRecs §\n\nPodcasts\n\nProgramming §\n\nCool Tools\n\nStudying §\n\nTextbooks\nCourses\n"},"notes/Linear-Algebra":{"title":"Linear Algebra","links":[],"tags":["seed"],"content":"\nThe manga guide to linear algebra: https://web.math.ucsb.edu/~jerryluo8/teaching/Fall2018-4A/The%20Manga%20Guide%20to%20Linear%20Algebra.pdf\n"},"notes/Manifesto":{"title":"Manifesto","links":[],"tags":["evergreen"],"content":""},"notes/Podcasts":{"title":"Podcasts","links":[],"tags":["seed"],"content":"\nStanford MLSys Seminar\n"},"notes/Productivity-Stack":{"title":"Productivity Stack","links":[],"tags":["evergreen"],"content":"My current productivity stack:\n\nRoam Research\n"},"notes/Research":{"title":"Research Whitepapers","links":[],"tags":["seed"],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitleLinkDynamoLinkMapReduceLinkTAOLinkThe Google File SystemLinkBigtableLinkCAP TheoremLinkKafkaLinkChubbyLinkLSM TreeLinkSpannerLinkConsistent HashingLinkOut of the Tar PitLinkRaft ConsensusLinkScaling MemcacheLinkBorgLink"},"notes/Startups-to-Follow":{"title":"Startups to Follow","links":[],"tags":["sapling"],"content":"Foundation Models\n\nOpenAI\n\n\nAnthropic\nAdept AI\nCohere\nSakana\nxAI\n\nRobotics\n\nFigure\n1X\nClone\nProsper\nReason\n\nOpen Models\n\nNous Research\nMistral\nggml\nSkunkworks\n\nAlignment\n\nConjecture\nApollo Research\n\nFinetuning\n\nGlaive\n\nHardware\n\nAtomic Semi\nEtched\nExtropic\nFactory\nImpulse Labs\nLightcell\nDirac\n\nWearables\n\nTab\nRewind\nRabbit\nHumane\n\nSearch\n\nPerplexity\nThe Browser Company\nExa\nGlean\n\nBrowser Automation\n\nMultiOn\nMinion\nInduced\n\nCode\n\nCursor\nContinuum\nCosine\nMagic\nGrit\nCodegen\nCodeSee\nMentat\nMorph\nTrace\n\nCloud Compute\n\nLambda\nModal\nExafunction\nSF Compute\nTogether\nAnyscale\nFal\nBanana\nBrev\n\nNeural\n\nNeuralink\nProphetic\nNeurable\nNeurosity\n\nProductivity\n\nEssential\nJulius\nHebbia\nNomic\nLindy\n\nEducation\n\nSynthesis\n\nCommunity\n\nOrdinary\nBuildspace\nCircle Labs\nPlexus Earth\n\nVector DBs\n\nChroma\nTurbopuffer\n\nSpeech\n\nElevenLabs\nSindarin Persona\n\nArt / Entertainment\n\nDingboard\nMidjourney\nPika\nSuno\nLuma Labs\nRunway\nLexica\nPlayground\nKrea\nStarlight Labs\nFigura Labs\nCartwheel\n"},"notes/Systems-Whitepapers":{"title":"Systems Whitepapers","links":[],"tags":["seed"],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitleLinkDynamoLinkMapReduceLinkTAOLinkThe Google File SystemLinkBigtableLinkCAP TheoremLinkKafkaLinkChubbyLinkLSM TreeLinkSpannerLinkConsistent HashingLinkOut of the Tar PitLinkRaft ConsensusLinkScaling MemcacheLinkBorgLink"},"notes/Textbooks":{"title":"Textbooks","links":[],"tags":["evergreen"],"content":"\nElements of Statistical Learning\n"},"notes/Topics-to-Explore":{"title":"Topics to Explore","links":[],"tags":["evergreen"],"content":"\nCompressed sensing\nStreaming and sketching algorithms\nNelson Elhage blog\n"},"notes/Transformers":{"title":"Transformers","links":[],"tags":["seed"],"content":"Transformers have been all the rage in the NLP community ever since GPT-3 was released and have recently become more well-known to the public after ChatGPT was released. I’m going to keep track of my favorite ways to learn about the Transformer architecture here.\nPapers §\n\nAttention is All You Need - Google Brain\nLanguage Models are Few-Shot Learners - OpenAI\nAligning Language Models to Follow Instructions - OpenAI\nLearning to summarize from human feedback - OpenAI\n\nBlog Posts §\n\nThe Illustrated Transformer - Jay Alammar\nThe Attention Mechanism - Jay Alammar\nThe Annotated Transformer - Sasha Rush\nTransformer Inference Arithmetic - Kipply\nTransformer Math 101 - EleutherAI\nTransformers from Scratch - Brandon Rohrer\nGPT in 60 lines of Numpy\n\nYouTube Videos §\n\nAttention is All You Need - Yannic Kilcher\nBuilding GPT from scratch - Andrej Karpathy\nIllustrated Guide to Transformers - Michael Phi\n\nCourses §\n\nTransformers United - Stanford CS25\nNLP with Deep Learning - Stanford CS224N\n"},"notes/Wish-List":{"title":"Wish List","links":[],"tags":["evergreen"],"content":"A list of things I’d eventually like to buy §"},"notes/index":{"title":"Notes","links":[],"tags":[],"content":"All of my notes live here! Still very much a work in progress!"},"notes/jpdf":{"title":"Joint Probability Density Function (JPDF)","links":[],"tags":[],"content":"Joint Probability Density Function (JPDF) for Random Matrices §\nThe Joint Probability Density Function (JPDF) is a fundamental concept in the study of random matrices, particularly in understanding the statistical properties of their eigenvalues. It describes how likely it is to find the eigenvalues of a random matrix within a particular range of values, providing insights into the distribution and correlation of these eigenvalues.\nDefinition §\nFor a random matrix, the JPDF of its eigenvalues λ1​,λ2​,…,λn​ is a function P(λ1​,λ2​,…,λn​) that gives the probability density of finding the matrix with its eigenvalues in the infinitesimally small intervals around λ1​,λ2​,…,λn​ simultaneously.\nImportance §\n\n\nSpectral Statistics: JPDF is crucial for understanding the spectral statistics of random matrices, i.e., the statistical properties of their eigenvalues. It serves as the basis for deriving other important statistical measures, such as the level spacing distribution and the spectral density.\n\n\nUniversality: Studies of JPDF have shown that certain statistical properties of eigenvalues are universal, meaning they do not depend on the specific details of the matrix distribution but only on its symmetry class (e.g., GOE, GUE, GSE).\n\n\nApplications: The analysis of JPDF and derived statistics has applications in various fields, including physics (quantum chaos, nuclear physics), mathematics (number theory), and complex systems (network theory, econophysics).\n\n\nCalculation §\nThe explicit form of the JPDF depends on the ensemble of random matrices considered (e.g., GOE, GUE, GSE) and is typically derived using methods from statistical physics and probability theory. For example, for the Gaussian Unitary Ensemble (GUE), the JPDF of eigenvalues can be expressed in a form that involves a Vandermonde determinant, indicating repulsion between eigenvalues:\nP(λ1​,λ2​,…,λn​)∝i&lt;j∏​(λi​−λj​)2exp(−i=1∑n​2σ2λi2​​),\nwhere σ2 is the variance of the entries of the matrix.\nConclusion §\nThe study of JPDF in random matrices reveals profound insights into the behavior of eigenvalues, reflecting both mathematical structures and potential physical phenomena. The patterns observed, such as eigenvalue repulsion and universal statistical behaviors, underscore the deep connections between random matrix theory and other areas of mathematics and physics."},"notes/linearalg":{"title":"Linear Algebra study","links":["tags/some2","notes/quaterion","notes/jpdf","notes/wigersurmise"],"tags":["some2"],"content":"Linear Algebra study §\nIn this webpage, i will be writing notes about my linear algebra study.\nIntroduction of Random Matrix Theory §\nRandom matrix theory is a branch of mathematical physics that studies the statistical properties of matrices. The theory is motivated by the observation that the eigenvalues of large random matrices often exhibit universal statistical behavior. This behavior is independent of the specific details of the matrix, such as its size, shape, or the distribution of its entries. Instead, the statistical properties of the eigenvalues depend only on the symmetry class of the matrix, which is determined by its symmetry properties.\nBooks §\nThe book i am using is Introduction of Random Matrices by Giacomo Livan, Marcel Novaes, Pierpaolo Vivo\nThere are couple of other books. But those are hard to understand. The include:\n\n\nA First Course in Random Matrix Theory\nfor Physicists, Engineers and Data Scientists by Potters and Bouchard \n\nSpecial thanks to @Cider for recommending this\n\n\n\nAn Introduction to Random Matrices\n\n\nYoutube Videos §\n\n\nRandom Matrices in Unexpected Places: Atomic Nuclei, Chaotic Billiards, Riemann Zeta#some2\n\n\nRandom Matrices: Theory and Practice - Lecture 1\n\n\nDig Deep §\nDefinitions §\nComplex matrix §\nAll real numbers are usually written as A+Bi, the same way matrix can be written as A+Bi where A and B are matrices.\nExample:\n[618−2i​2+3i4i​]=[618​20​]+[0−2​34​]\nHermitian matrix §\nA matrix is Hermitian if it is equal to its own conjugate transpose. In other words, a matrix A is Hermitian if A=AH. Proof here\nAlso remember that λ1​≥λ2​≥⋯≥λn​\nExample:\n[21+i​1−i3​]=&gt;[21+i​1−i3​]=&gt;[21+i​1−i3​]\nGaussian Orthogonal Ensemble (GOE) §\n\nSymmetry: Real symmetric matrices. This means that the matrix is equal to its transpose (A=AT), and all elements are real numbers.\nKey Property: The matrices are invariant under orthogonal transformations. If O is an orthogonal matrix (OOT=I), then OAOT is also in the GOE if A is.\nApplications: Models real symmetric systems with time-reversal symmetry, such as certain Hamiltonians in physics.\n\nGaussian Unitary Ensemble (GUE) §\n\nSymmetry: Complex Hermitian matrices. This means the matrix is equal to its conjugate transpose (A=A†), with complex entries.\nKey Property: The matrices are invariant under unitary transformations. If U is a unitary matrix (UU†=I), then UAU† is also in the GUE if A is.\nApplications: Used to model systems without time-reversal symmetry, which is common in quantum mechanics for systems experiencing a magnetic field.\n\nGaussian Symplectic Ensemble (GSE) §\n\nSymmetry: Self-dual quaternion matrices, which can be thought of as a generalization involving quaternions, a type of hypercomplex number.\nKey Property: The matrices are invariant under symplectic transformations. This ensemble captures the properties of systems with a specific kind of time-reversal symmetry and spin-orbit interaction.\nApplications: Models systems that have both time-reversal symmetry and strong spin-orbit interaction effects, which are less common but relevant in certain areas of condensed matter physics.\n\nKey Differences §\n\nMathematical Structure: The primary difference lies in the mathematical structure of the matrices (real vs. complex vs. quaternion) and their symmetry properties (orthogonal vs. unitary vs. symplectic).\nPhysical Applications: Each ensemble is suited to model different physical systems based on their symmetry properties—GOE for real symmetric systems with time-reversal symmetry, GUE for complex Hermitian systems without time-reversal symmetry, and GSE for systems with peculiar time-reversal symmetry and spin interactions.\nStatistical Properties: The level spacing distributions (the distribution of distances between adjacent eigenvalues) differ among the three, reflecting the underlying symmetries and interactions in the modeled physical systems.\n\nLearn about quaternions:  Quaterions and its applications\nStudy §\nWe have a way to generate random standard normal matrix which is symmetric. The code is as follows:\nimport numpy as np\n\ndef random_standard_normal_matrix(m,n):\n    A = np.random.normal(0, 1, size=(m, n))\n    return  np.tril(A) + np.triu(A.T, 1)\n\nTo get the eigenvalues of the matrix, we can use the following code:\nfrom numpy.linalg import eig,eigvals\n\neigvals(A)\n\nSince, these are random matrices, we can generate multiple matrices and then calculate all the stats of these matrices. This is refered to as an ensemble of matrices.\nGenerating the eigenvalues of 100000 matrices of size 3x3:\neigen = []\nfor i in range(100000):\n    eigen += list(eigvals(random_standard_normal_matrix(3,3)))\n\nPlotting the distribution of the standard normal matrix, we get this.\n\nNow, you can see something intresting here. We have N(μ,σ2) distribution for this new distributions of eigen values.\nThis is called the Wigner’s semi-circle law. The eigenvalues of a random matrix are distributed in a semi-circle. This is a very important property of the eigenvalues of a random matrix.\nAnother important part is how do we compare two ensembles of matrices. This is done by calculating the graph above. But there is a problem. That is for 3×3 matrix. If we increase the size of the matrix 6×6, the graph will look different for the same distribution with the same mean and variance we get a wider graph\n\nTo normalize this: we run the following code:\nBefore Normalization\n\nBefore\nAfter Normalization\n\nLearn, the JPDF better here: Joint Probability Density Function (JPDF) for Random Matrices\nThere is an another intresting conclusion\nWigner’s Surmise §\nProbability of sampling two eigenvalues ’very close’ to each other (s → 0) is very small. This delta for any two eigenvalues can be calculated by\n2s​e−4s2​\nVisual representation of this is as follows:\n\ntake a more detailed view Wigner’s Surmise\nThe important (generic) feature is that the eigen vectors are not independent: their jpdf does not in general\nfactorize. The most striking incarnation of this property is the so-called level repulsion (as in Wigner’s\nsurmise): the eigenvalues of random matrices generically repel each other, while independent variables do\nnot have this property. This is a very important property of the eigenvalues of a random matrix.\nFor the standard normal R3 we get these correlation values:\n​1−0.59881399−0.62568476​−0.59881.0.43375837​−0.62560.433758371.​​\nWhich shows these are correlated."},"notes/quaterion":{"title":"Quaternion","links":[],"tags":[],"content":"Quaternion §\nA quaternion is a type of hypercomplex number that extends complex numbers. It is typically represented as:\nq=a+bi+cj+dk\nwhere:\n\na, b, c, and d are real numbers,\ni, j, and k are the fundamental quaternion units.\n\nQuaternions have properties that make them useful for representing rotations and orientations in three-dimensional space. They avoid the singularity and ambiguity problems of Euler angles and are more compact than rotation matrices.\nWATCH THIS: https://eater.net/quaternions\nOperations with Quaternions §\nQuaternions support addition, subtraction, multiplication, and division, but multiplication is not commutative:\nij=k,ji=−k,jk=i,kj=−i,ki=j,ik=−j,i2=j2=k2=ijk=−1\nQuaternion Conjugate §\nThe conjugate of a quaternion q=a+bi+cj+dk is defined as:\nq∗=a−bi−cj−dk\nNorm of a Quaternion §\nThe norm of a quaternion is given by:\n∥q∥=a2+b2+c2+d2​\nQuaternion Inverse §\nThe inverse of a quaternion q is defined as:\nq−1=∥q∥2q∗​\nQuaternion Matrix §\nA quaternion matrix is a matrix where each element is a quaternion. These matrices can represent complex transformations in higher-dimensional spaces and are used in various fields, including theoretical physics, computer graphics, and robotics.\nApplications §\nQuaternion matrices are particularly useful for:\n\nRotating points in three-dimensional space,\nInterpolating orientations (slerp),\nSimulating rigid body dynamics.\n\nTheir ability to represent rotations without suffering from gimbal lock makes them invaluable in 3D computer graphics and aerospace engineering."},"notes/wigersurmise":{"title":"Wigner’s surmise","links":[],"tags":[],"content":"Wigner’s surmise §\nConsider a 2 x 2 GOE matrix ( H_s = \\left( \\begin{array}{cc}\nx_1 &amp; x_3 \\\nx_3 &amp; x_2 \\\n\\end{array} \\right) ) with ( x_1, x_2 \\sim \\mathcal{N}(0, 1) ) and ( x_3 \\sim \\mathcal{N}(0, 1/2) ). What is the pdf ( \\rho(s) ) of the spacing ( s = \\lambda_2 - \\lambda_1 ) between its two eigenvalues (( \\lambda_2 &gt; \\lambda_1 ))?\nThe two eigenvalues are random variables, given in terms of the entries by the roots of the characteristic polynomial\nλ2−Tr(Hs​)λ+det(Hs​),\ntherefore ( \\lambda_{1,2} = (x_1 + x_2 \\pm \\sqrt{(x_1 - x_2)^2 + 4x_3^2})/2 ) and ( s = \\sqrt{(x_1 - x_2)^2 + 4x_3^2} ).\nBy definition, we have\nρ(s)=∫−∞∞​∫−∞∞​∫−∞∞​2π​2π​π​e−21​x12​e−21​x22​e−x32​​δ(s−(x1​−x2​)2+4x32​​)dx1​dx2​dx3​.\nChanging variables as\n⎩⎨⎧​x1​−x2​=rcosθ2x3​=rsinθx1​+x2​=ψ​⇒⎩⎨⎧​x1​=2rcosθ+ψ​x2​=2ψ−rcosθ​x3​=2rsinθ​​\nand computing the corresponding Jacobian\nJ=det​∂r∂x1​​∂r∂x2​​∂r∂x3​​​∂θ∂x1​​∂θ∂x2​​∂θ∂x3​​​∂ψ∂x1​​∂ψ∂x2​​∂ψ∂x3​​​​=det​cos2θ​−cos2θ​sin2θ​​−2rsin2θ​​2rsin2θ​​2rcos2θ​​​21​21​0​​=−4r​,\nOne obtains\n= \\frac{\\sqrt{4\\pi s}}{8\\pi^{3/2}} \\int_0^{2\\pi} d\\theta e^{-\\frac{s^2}{2}\\left[ \\frac{\\cos^2 \\theta}{2} + \\frac{\\sin^2 \\theta}{2} \\right]} \n= \\frac{s}{2}e^{-\\frac{s^2}{4}}.$$"},"tags/dreams":{"title":"Dreams","links":[],"tags":[],"content":""},"tags/evergreen":{"title":"Evergreen","links":[],"tags":[],"content":""},"tags/sapling":{"title":"Sapling","links":[],"tags":[],"content":""},"tags/seed":{"title":"Seed","links":[],"tags":[],"content":""}}
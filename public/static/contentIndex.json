{"gallery":{"title":"Gallery","links":[],"tags":["evergreen"],"content":"Collection of my favorite pictures\n\n    \n\n\nOng-Ard Satrabhandhu is a Thai architect whose work is known for its masterful combination of classical principles and traditional Thai qualities\n\n\n    \n\n\nPuppies, by Maruyama Ōkyo, ca. 1790\n\n\n    \n\n\nElephant Family, by Marc Allante\n\n\n    \n\n\nUnconquered sol on Twitter\n\n\n    \n\n\nGhost in the shell Anime\n\n\n    \n\n\nKorean Buddhist Library\n\n\n    \n\n\nBuffalo Cops in Thailand\n\n\n    \n    \n\nPrometheus in chains by Frantisek Kupka\n\n\n    \n\n\nOwl annoyed by a squirrel\n\n\n    \n\n\nRadha Krishna\n\n\n    \n\n\nMoon like Saturn\n\n\n    \n\n\nGodess Saraswati\n\n\n    \n\n\nThe Ruins of Pompeii art print by Filippo Palizzi\n\n\n    \n\n\nHouse in the hills\n\n\n    \n\n\nReplicant and an Angel\n\n\n    \n\n\nBodhidharma by Yoshitoshi\n\n\n    \n\n\nEnryaku-ji Temple, Kyoto\n\n\n    \n\n\nCute band of Monkeys playing in the snow\n\n\n    \n\n\nRiverside Church during my studies at Columbia University\n\n\n    \n\n\nTomb of Tutankhamun\n\n\n    \n\n\nOil rig in the North Sea\n\n\n    \n\n\nCute pigeon painting\n\n\n    \n\n\nSpace Shuttle and F-16\n\n\n    \n\n\nView of a Nebula from a space-ship window\n\n\n    \n\n\nNeat networking in a Data Center\n\n\n    \n\n\nRajput Horse\n\n\n    \n\n\nConcordian eclipse\n\n\n    \n\n\nTaj Lake Palace Hotel, Udaipur\n\n\n    \n\n\nMinoan Women by Bakst\n\n\n    \n\n\nOil from Blade Runner\n"},"index":{"title":"Eternal Horizons","links":["gallery","lighl16","notes","tags/dreams","tags/seed","tags/sapling","tags/evergreen","notes/linearalg","notes/transformerdeep"],"tags":[],"content":"\nA fault in the space-time continuum where two normally distant points of space touch one another\n\n\n    \n\nWelcome to my blog, I will mainly talk about the NLP, Complexity theory, re-engineering products, modern-day Hindutva’s deontological ethics, lack of meaning in a world approaching AGI and computational search problems.\nEmbark on this voyage with me as we seek to discover our unique silhouettes in a world that often feels devoid of true significance. Hope you leave my blog with a sense of meaning and a little bit of inspiration.\nYou can browse on the left, explore the graph on the right, or search the whole site with ⌘+K.\nMeanwhile, take a look at my gallery of my favorite pictures and my project for the month\nAll notes are in one of four states:\n\nDreams: messages from the deep\nSeed: brief ideas\nSapling: fully-formed thoughts\nEvergreen: permanent, constantly growing\n\nThank you Ishan and Wayne for the template\nProjects working concurrently §\n\nLight L16 re-engineering\nLinear Algebra recap specifically RMTs\nTransformers from first-principles\n\nUpcoming projects §\n\nRecreate the seed finding algorithm for tall cactus finding in Minecraft\nDocument the exploratory of voyager in minecraft and how it can be used\nPerlin Noise and how to make it more human like\n\nContact me §\nIf you do have any suggestions or areas we can work together. You can contact me at ss6365 [at] columbia.edu or saisam2021 [at] gmail.com or @humintmax"},"lighl16":{"title":"Light L16 Re-engineering","links":["light/lri","light/ransac","light/sift","light/bayer","light/vignetting","light/tof","light/10bit"],"tags":["seed"],"content":"In early 2018, Light.co released a groundbreaking camera. It was claimed to be engineering marvel. It takes 16 different images and using computational photogrammetry combines the 16 different images into one. Unfortunately the cameras software was badly executed. In this blog post, we will think of ways to re-engineer the device and try to recreate the software using the same methods.\nAll this work, wouldn’t be possible without gennyble and all the members in light.co discord server.\nHow it works under the hood ? §\nL16 simulates all the focal lengths in between 28mm, 70mm, and 150mm by combining data from multiple camera modules. So instead of digitally zooming in on the 28mm image to make it look like it was shot at 40mm, it’s replicating that focal length by stitching images together on the fly.\n\n    \n\nSince the company was acquired by John Deere, all the support for the device is non existant. We will have to understand all the basic parts of all the software by re-engineering every aspect of it.\nWe have mainly three parts which we will have to figure out. These include:\n\nLumen Software used for merging\nRaw file generated .LRI\nCamera APK and C++ Files\n\nSince, this is a very engineering heavy project and relies on a lot of people. We have the following subpages, to better understand terms and files. Please click on each link to view.\n\nRanSAC\nFoward Matrix and Color Matrix\nIlluminant type\nDe-Mosaic\nSIFT\nProtoBuff\nGamma\nBayer Jpeg\nVignetting characterization\nToFCalibration\nTiFF files\n10 Bit Images\n\nDetailed Steps to be completed §\nRaw file creation §\nThe current process deals with .lri format. In the future, each individual image needs to be converted to .RAW format image which can be editied\nStiching §\nThe whole stiching pipeline involves a lot of stages these are:\nEnhance Feature Matching: §\n\nPreprocess images to enhance features: Applying filters like edge enhancement can make feature points more distinct.\nTweak feature detection parameters: If using custom stitching algorithms (beyond OpenCV’s built-in Stitcher), adjusting parameters of feature detectors (e.g., SIFT, SURF) may yield better matches.\n\nImprove Alignment: §\n\nManual alignment: For a small number of images, manually selecting matching points can improve the homography estimation.\nRANSAC for Homography Estimation: Ensure the RANSAC (Random Sample Consensus) algorithm is used to find the best homography matrix that aligns images. It’s robust against outliers.\n\nSeam Correction: §\n\nSeam finding algorithms: Utilize seam finding and blending algorithms to minimize visible seams. OpenCV offers functions like createMultiBandBlender() which might produce better results than the default blending options in the Stitcher class.\nManual editing: In cases where automated solutions fail, manual editing in photo editing software might be necessary to correct mismatches or blend seams.\n\nExposure and Color Correction: §\n\nAdjust exposure and color balance: Before stitching, adjust the images to match in terms of exposure and color balance to reduce visual discrepancies.\nApply gain compensation: Some stitching libraries offer gain compensation to equalize the brightness and color across stitched images.\n\nAdvanced Techniques and research: §\n\nHomography refinement: After an initial stitch, refine the homography matrices by minimizing the discrepancy between the overlapping areas.\nMulti-scale stitching: Perform stitching at multiple scales or resolutions to ensure both large structures and fine details are well aligned.\n"},"light/10bit":{"title":"10Bit","links":[],"tags":[],"content":"A 10-bit image refers to the bit depth of an image, which is a measure of the number of bits used to represent the color of a single pixel. In a 10-bit image, each color channel of a pixel is represented with 10 bits of data. This means that for a standard RGB (Red, Green, Blue) color model, each channel can display 2^10 or 1024 different shades of intensity.\nCompared to the more common 8-bit images, which can display 256 shades per channel, 10-bit images can display a much wider range of colors. This results in smoother gradients and more detailed color representation, reducing the risk of banding in images.\nHere’s a quick comparison in terms of potential colors:\n\n8-bit per channel: 256 levels per channel, resulting in 16.7 million possible colors (256^3).\n10-bit per channel: 1024 levels per channel, resulting in over 1 billion possible colors (1024^3).\n\nThis increased color depth is particularly beneficial in professional photo and video editing, where it allows for more precise adjustments without losing detail or causing posterization."},"light/bayer":{"title":"Bayer JPEG","links":[],"tags":[],"content":"All of these investigations are from @gennyble, Please go say hi!\nThe BayerJPEG is a strange format used by the Light L16… sometimes. We don’t yet know when it switches from it’s normal packed 10-bit raw format to this, or why.\nBayerJPEG Header §\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsizetypemeaning4 bytesStringMagic Number “BJPG”4 bytesu32Format type  0: colour  1: for monochrome4 bytesu32Length of Jpeg 04 bytesu32Length of Jpeg 14 bytesu32Length of Jpeg 24 bytesu32Length of Jpeg 31552 bytesunknown\nForamt Type: Monochrome\nJpeg0 contains a full resolution grayscale image\nFormat Type: Colour\nThe bayered image is split across the four Jpeg, one\nfor each colour location.\nI.E. an image from the ar1335 sensor, color filter bggr, you’d get\n\n1 jpeg for the blue channel\n2 jpeg for each green location\n1 jpeg for the red channel\n\nIt’s not currently known if these are in the order you’d expect.\nConsiderations\nWhen the L16 decides to use BayerJPEG, it has to save four copies of each frame. A JPEG is limited to a bit depth of eight, but the sensors output 10-bit data. In order to not loose 75% of the precision, they seemingly divide the image into fours and expect you to sum them later."},"light/demosaic":{"title":"demosaic","links":[],"tags":[],"content":""},"light/forward":{"title":"forward","links":[],"tags":[],"content":""},"light/gamma":{"title":"gamma","links":[],"tags":[],"content":""},"light/lri":{"title":"LRI File","links":["light/bayer"],"tags":[],"content":"All of these investigations are from @gennyble, Please go say hi!\nAnatomy of an LRI §\nThe file is made up of many blocks, usually 10 or 11 but cases of 40 have occurred.\nBlocks start with a header and contain some data. There is always a protobuf message within that data, and sometimes stuff like the images themselves.\nBlock Header §\nThe header is 32 bytes long, uses little-endian, and goes as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbytestypemeaning4-Signature: “LELR”8u64block length8u64message offset from block start4u32message length1u8message type, see below7-reserved\nmessage to mean the protobuf message\nMessage Type\n0: LightHeader ([proto][lh-proto])\n1: ViewPreferences ([proto][vp-proto])\n2: GPSData ([proto][gps-proto])\n\nGo through all the blocks for all the details as the information might be split across multiple branches\nhighlighting key parts §\nblocks have messages and that’s pretty much all you need to know. you can look at the protobuf definitions and pretty readily build a parser, but there are some things i’d like to mention, too.\nLightHeader §\nThe most important header and frustratingly fractured between multiple blocks.\nRAW Images §\nWhat we’re all here for, maybe.\nzero or more CameraModule  are collected in the modules. In a CameraModule we see a sensor_data_surface of type Surface (see line 33 of the CameraModule proto).\n\n\nstart might indicate a crop, but has always been (0,0) in my experience.\nsize gives the width/height of the image.\ndata_offset is the start of the image from the beginning of the block (meaning: it includes the length of the header).\nformat indicates how we’re meant to interpret the image data. It can be a few different things, but i’ve only seen RAW_BAYER_JPEG and RAW_PACKED_10BPP.\nrow_stride gives you the number of bytes per row the image takes up. Multiply this by the width to get the size of the image (except Bayer JPEG; see below)\n\nLet’s talk about Bayer JPEG. §\nWe don’t currently understand why the L16 makes these, just that it does. If it’s from a colour sensor, you’ll get four half-res JPEG (one for each bayer position). If it’s monochrome, you’ll get one full-res JPEG. For more information go here: Bayer Jpeg.\nIn either colour-case, the row_stride in the sensor_data_surface will be 0. You’ll have to parse the Bayer JPEG header to get the length of the sensor’s image data.\nthat’s enough BayerJPEG §\nGoing back to CameraModule, there’s some more important data for image interpretation. You’ll want the id which indicates which camera took the exposure. We can map this to a sensor model later! Grab sensor_bayer_red_override while you’re at it. It’ll help with figuring out what CFA we need to use for debayering.\nBack in the LightHeader now we’ll go to hw_info, type HwInfo , then to camera which is a CameraModuleHwInfo (described on line 8 in the HwInfo definition). From this we can associate a CameraID, id, with a SensorType, sensor. It might be good to note here that there are quite a few SensorType defined, but i’ve only ever seen AR1335 and it’s monochrome variant. Perhaps the others were used in development?\n\n\n\tnote on the above\n\t\n\t\tI&#039;m not sure how necessary it is to make this map. Is it ridiculous to assume that the CameraID are consistent between L16 and they they are the same SensorType? This could very well be hard coded with very little harm.\n\t\n\nsensor_bayer_red_override §\nAs far as I can tell this tells us how to shift the CFA for the specific camera. I don’t know why it’s different; perhaps it’s cropped before writing to disk?\nAnyway, the x/y you’re given seem to map to where Red should be in the 2x2 array. For example, if you have a BGGR cfa and your override is x=1 y=0, you should end up with GRBG (See the ascii diagram below).\nBGGR cfa                 GRBG cfa\n\nB G B G                  G R G R\nG R G R  -&gt; override -&gt;  B G B G\nB G B G     x:1, y:0     G R G R\nG R G R                  G B G B\n\nColour Data §"},"light/lumen":{"title":"lumen","links":[],"tags":[],"content":""},"light/protobuff":{"title":"protobuff","links":[],"tags":[],"content":""},"light/ransac":{"title":"Ransac","links":[],"tags":[],"content":"The RANSAC (Random Sample Consensus) algorithm is a robust estimation method used to fit a mathematical model to a dataset that contains outliers. It is widely used in computer vision for tasks such as estimating the fundamental matrix in stereo vision, finding homographies between image pairs in panorama stitching, and fitting geometric shapes to sets of points. RANSAC works by repeatedly selecting a random subset of the original data to estimate model parameters, then validating this model against the entire dataset to find the best-fitting model that has the highest number of inliers.\nThe general steps of RANSAC are :\n\nSelect a Random Subset: Randomly select a minimal subset of points necessary to estimate the model parameters.\nEstimate the Model: Estimate the model parameters using this subset.\nIdentify Inliers: Determine which points from the entire dataset fit this model within a certain threshold (these points are considered inliers).\nRepeat: Repeat steps 1-3 for a predefined number of iterations, each time updating the best-fitting model if the current iteration has more inliers.\nBest Model: After all iterations, choose the model with the highest number of inliers.\nRANSAC is particularly useful because it is capable of providing a good estimation even when the data contains a significant number of outliers.\n\nExample in OpenCV: Estimating a Homography with RANSAC §\nBelow is an example of using RANSAC in OpenCV to estimate a homography between two images. This is commonly used in applications like image stitching or when you want to understand the transformation between two scenes.\nimport cv2\nimport numpy as np\n\n# Load your images\nimage1 = cv2.imread(&#039;image1.jpg&#039;)\nimage2 = cv2.imread(&#039;image2.jpg&#039;)\n\n# Convert images to grayscale\ngray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\ngray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n\n# Initialize SIFT detector\nsift = cv2.SIFT_create()\n\n# Find keypoints and descriptors with SIFT\nkeypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\nkeypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n\n# Create a BFMatcher object\nbf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n\n# Match descriptors\nmatches = bf.match(descriptors1, descriptors2)\n\n# Sort them in the order of their distance\nmatches = sorted(matches, key=lambda x:x.distance)\n\n# Draw first 10 matches\nimg_matches = cv2.drawMatches(image1, keypoints1, image2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n\n# Extract location of good matches\npoints1 = np.zeros((len(matches), 2), dtype=np.float32)\npoints2 = np.zeros_like(points1)\n\nfor i, match in enumerate(matches):\n    points1[i, :] = keypoints1[match.queryIdx].pt\n    points2[i, :] = keypoints2[match.trainIdx].pt\n\n# Find Homography\nH, status = cv2.findHomography(points1, points2, cv2.RANSAC)\n\n# Use the Homography Matrix to warp the images\nresult = cv2.warpPerspective(image1, H, (image1.shape[1] + image2.shape[1], image1.shape[0]))\nresult[0:image2.shape[0], 0:image2.shape[1]] = image2\n\n# Display the stitched image\ncv2.imshow(&#039;Result&#039;, result)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\nThis code performs the following steps:\n\nLoads two images and converts them to grayscale.\nDetects SIFT keypoints and computes descriptors for both images.\nUses a Brute-Force Matcher to match the descriptors between the two images.\nUses RANSAC to estimate a homography matrix that best aligns the two images.\nWarps one image onto the other using the estimated homography to demonstrate how they align.\n"},"light/sift":{"title":"SIFT","links":[],"tags":[],"content":"The SIFT (Scale-Invariant Feature Transform) algorithm in OpenCV is a feature detection algorithm used in computer vision tasks to detect and describe local features in images. It’s useful for tasks like object recognition, image stitching, and tracking because it’s invariant to scale, rotation, and partially invariant to change in illumination and 3D viewpoint.\nHere’s a simple example of how to use the SIFT algorithm in OpenCV to detect keypoints in an image:\npip install opencv-python opencv-contrib-python\n\nimport cv2\nimport numpy as np\n\n# Load the image\nimage = cv2.imread(&#039;path_to_your_image.jpg&#039;)\n\n# Convert the image to grayscale\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# Initialize the SIFT detector\nsift = cv2.SIFT_create()\n\n# Detect keypoints in the image\nkeypoints = sift.detect(gray, None)\n\n# Draw the keypoints on the image\nkeypoint_image = np.zeros_like(image)\nkeypoint_image = cv2.drawKeypoints(image, keypoints, keypoint_image, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\n# Display the image with keypoints\ncv2.imshow(&#039;SIFT Keypoints&#039;, keypoint_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\nIn this code:\n\nWe load an image and convert it to grayscale because SIFT operates on single-channel images.\nWe initialize the SIFT feature detector using cv2.SIFT_create().\nWe detect keypoints in the grayscale image using the detect method.\nWe draw these keypoints on the original image using cv2.drawKeypoints for visualization.\nFinally, we display the image with keypoints.\n"},"light/tof":{"title":"ToF Calibration","links":[],"tags":[],"content":"Time-of-Flight Calibration, relates to the calibration process for Time-of-Flight (ToF) cameras or sensors. ToF technology measures the time it takes for light to travel from the sensor to the subject and back, allowing the device to accurately map the distance or depth of objects in its field of view. This technology is used in various applications, including 3D imaging, augmented reality (AR), robotics, and security systems.\nToF Calibration is crucial for ensuring that the depth information captured by the ToF sensor is accurate. Here’s what the calibration process typically involves:\nDistance Calibration §\nThis involves adjusting the sensor’s measurements so that they accurately reflect the true distances. Due to various factors such as temperature, the speed of light through different media, or manufacturing variances, the raw measurements from a ToF sensor may not perfectly match the actual distances. Calibration ensures that the sensor’s output corresponds accurately to the real-world distances.\nGeometric Calibration §\nSince ToF sensors are often used in conjunction with other imaging sensors (e.g., RGB cameras), geometric calibration is necessary to align the depth data with data from other sensors. This ensures that the depth information correctly matches up with the corresponding visual information, which is essential for applications like augmented reality, where virtual objects must accurately overlay on the real world.\nReflectivity Calibration §\nReflectivity or intensity calibration accounts for how different surfaces reflect light differently, affecting the sensor’s ability to measure distance accurately. Some surfaces might absorb more light, while others reflect it more, leading to variations in the measured times that do not correspond to actual distance differences. Calibration processes adjust for these variations to ensure consistent depth measurements across different types of surfaces.\nError Correction §\nCalibration also involves identifying and correcting for systematic errors in the sensor’s measurements, such as those caused by the sensor’s internal components or external environmental factors.\nCalibrating a ToF sensor can involve using calibration targets at known distances and positions, environmental controls to minimize external factors, and sophisticated software algorithms to process the calibration data. Proper calibration is essential for the accurate functioning of ToF-based systems, especially in applications requiring high precision in distance and depth measurements."},"light/vignetting":{"title":"Vignetting","links":[],"tags":[],"content":"Vignetting characterization refers to the process of identifying and measuring vignetting, which is a reduction in image brightness or saturation at the periphery compared to the image center. Vignetting is a common optical phenomenon that can occur for various reasons, such as the physical properties of lens elements, lens hoods, or filters that block some light from reaching the outer portions of the image sensor. It can also be caused by the natural falloff of light from the center to the edge of the image field due to the angle at which light rays enter the lens elements.\nIn the context of camera calibration and image processing, vignetting characterization is crucial for several reasons:\n\n\nCorrection in Post-Processing: By understanding the vignetting characteristics of a specific camera-lens setup, software can correct for this effect, leading to more uniformly exposed images. This is especially important in professional photography and high-precision applications like photogrammetry, where image consistency and accuracy are crucial.\n\n\nEnhanced Image Quality: Correcting vignetting can improve the overall aesthetic and technical quality of images, making them more appealing and accurate representations of the scene.\n\n\nLens and Camera Evaluation: Vignetting characterization can be used to evaluate the performance of camera lenses under different conditions, such as various aperture settings, focal lengths, and focus distances. This information can be valuable for lens manufacturers during development and for photographers when selecting equipment for specific use cases.\n\n\nThe process typically involves capturing images of uniform, brightly lit surfaces or specialized test charts at different settings, then analyzing these images to quantify the vignetting effect. This analysis can be done using various image processing software tools. The characterization might result in a vignetting profile that describes how the vignetting effect varies across the image field and under different shooting conditions. This profile can then be used to automatically correct images taken under similar conditions, significantly improving the image quality or preparing images for further analysis or processing."},"notes/Books":{"title":"Sci-Fi Books","links":["notes/geometrymeaning"],"tags":["evergreen"],"content":"Some pages of books §\n\nGeometry of Meaning by Peter Gardenfors\n\n"},"notes/Idea-List":{"title":"Idea List","links":[],"tags":["dreams"],"content":"Open ideas, if you would want to collaborate on any of these, please let me know. I have done some progress in them but not detailed enoughs\n\nLight L16 stiching algorithm\n\nReplace the method of laplacian blending with a more advanced method. Specifically try Alpha blending and cyclendrical blending\n\n\nAutomatic Minecraft Player and Crafter\n\nShow videos of builds and recreate them in your world\n\n\nNvidia Omniverse 3D asset creation\n\nCreate a 3D assets and place it in a 3d Location\nProgramatic world generation\nSora doesn’t follow geometries, omniverse can help fix\n\n\nObjaverse Search\n\n3D asset search is broken. It doesn’t have any open source ranking algorithm.\n\n\nRAG at scale\n\nProblem: RAG doesn’t scale well. The embeddings overlap and the search return non-relevant results\n\n\nTokenizer:\n\nBPE has some limitations. A particularly common word may get a unique BPE while a longer word will be encoded as 2 or 3 BPEs, and a completely novel word will be encoded by letter BPE as a fallback.\nRead some blogs from Gwern, Karpathy and Nostalgebraist\nA lot of ideas to borrow from the hutter prize.\nTrain couple of GPT models with different tokenizers and see how they perform. 100$ per run.\n\n\nTransformers for maths:\n\nThere are a lot of problems with math tokenization, math operations and math\n\n\n"},"notes/Internet-Gems":{"title":"Fun blogs","links":[],"tags":["evergreen"],"content":"Hardware §\n\nhttps://siboehm.com/\nhttps://leimao.github.io/\nhttps://highscalability.com/\n\nSoftware §\n\nhttps://kipp.ly/\n\nLife lessons §\n\nhttps://www.benkuhn.net/\n"},"notes/Library":{"title":"Library","links":["notes/Podcasts","Cool-Tools","notes/Textbooks","Courses"],"tags":["evergreen"],"content":"Have a nice read, hope you find something you like :)\nRecs §\n\nPodcasts\n\nProgramming §\n\nCool Tools\n\nStudying §\n\nTextbooks\nCourses\n"},"notes/Manifesto":{"title":"Manifesto","links":[],"tags":["evergreen"],"content":"WORK IN PROGRESS §\nMy whole phil:\n\nComplex systems penalizes massive step functions\nMost changes around you have increase in variance but mode stays the same\nComplexity increases with time but not due to the mean but due to the variance\nComputational beats analytical solutions\nNever bet against/ always bet with self-organized criticality\nPeople confuse multiple exponentials with log-normal\nHuman behaviour corresponds to a Kuramoto model of reality\nPercolation theory is under-studied massively\n\nStrong opinions weakly held §\n\nCurrent LMs representations focus on diachronic meaning but not in the synchronic meaning.\n"},"notes/Podcasts":{"title":"Podcasts","links":[],"tags":["seed"],"content":"\nStanford MLSys Seminar\n"},"notes/Productivity-Stack":{"title":"Productivity Stack","links":[],"tags":["evergreen"],"content":"My current productivity stack:\n\nRoam Research\n"},"notes/Research":{"title":"Research Whitepapers","links":[],"tags":["seed"],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitleLinkDynamoLinkMapReduceLinkTAOLinkThe Google File SystemLinkBigtableLinkCAP TheoremLinkKafkaLinkChubbyLinkLSM TreeLinkSpannerLinkConsistent HashingLinkOut of the Tar PitLinkRaft ConsensusLinkScaling MemcacheLinkBorgLink"},"notes/Startups-to-Follow":{"title":"Startups to Follow","links":[],"tags":["sapling"],"content":"Borrowed from Ishan’s blog post §\nFoundation Models\n\nOpenAI\n\n\nAnthropic\nAdept AI\nCohere\nSakana\nxAI\n\nRobotics\n\nFigure\n1X\nClone\nProsper\nReason\n\nOpen Models\n\nNous Research\nMistral\nggml\nSkunkworks\n\nAlignment\n\nConjecture\nApollo Research\n\nFinetuning\n\nGlaive\n\nHardware\n\nAtomic Semi\nEtched\nExtropic\nFactory\nImpulse Labs\nLightcell\nDirac\n\nWearables\n\nTab\nRewind\nRabbit\nHumane\n\nSearch\n\nPerplexity\nThe Browser Company\nExa\nGlean\n\nBrowser Automation\n\nMultiOn\nMinion\nInduced\n\nCode\n\nCursor\nContinuum\nCosine\nMagic\nGrit\nCodegen\nCodeSee\nMentat\nMorph\nTrace\n\nCloud Compute\n\nLambda\nModal\nExafunction\nSF Compute\nTogether\nAnyscale\nFal\nBanana\nBrev\n\nNeural\n\nNeuralink\nProphetic\nNeurable\nNeurosity\n\nProductivity\n\nEssential\nJulius\nHebbia\nNomic\nLindy\n\nEducation\n\nSynthesis\n\nCommunity\n\nOrdinary\nBuildspace\nCircle Labs\nPlexus Earth\n\nVector DBs\n\nChroma\nTurbopuffer\n\nSpeech\n\nElevenLabs\nSindarin Persona\n\nArt / Entertainment\n\nDingboard\nMidjourney\nPika\nSuno\nLuma Labs\nRunway\nLexica\nPlayground\nKrea\nStarlight Labs\nFigura Labs\nCartwheel\n"},"notes/Systems-Whitepapers":{"title":"Systems Whitepapers","links":[],"tags":["seed"],"content":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTitleLinkDynamoLinkMapReduceLinkTAOLinkThe Google File SystemLinkBigtableLinkCAP TheoremLinkKafkaLinkChubbyLinkLSM TreeLinkSpannerLinkConsistent HashingLinkOut of the Tar PitLinkRaft ConsensusLinkScaling MemcacheLinkBorgLink"},"notes/Textbooks":{"title":"Textbooks","links":[],"tags":["evergreen"],"content":"\nElements of Statistical Learning\n"},"notes/Topics-to-Explore":{"title":"Topics to Explore","links":[],"tags":["evergreen"],"content":"\nTerreance Tao Blog\nDominic Cumming substack\nGrey Mirror ( haven’t read in a year)\n"},"notes/Transformers":{"title":"Transformers","links":["notes/transformerdeep"],"tags":["seed"],"content":"Transformers have been all the rage in the NLP community ever since GPT-3 was released and have recently become more well-known to the public after ChatGPT was released. I’m going to keep track of my favorite ways to learn about the Transformer architecture here.\nPapers §\n\nAttention is All You Need - Google Brain\nLanguage Models are Few-Shot Learners - OpenAI\nAligning Language Models to Follow Instructions - OpenAI\nLearning to summarize from human feedback - OpenAI\n\nBlog Posts §\n\nThe Illustrated Transformer - Jay Alammar\nThe Attention Mechanism - Jay Alammar\nThe Annotated Transformer - Sasha Rush\nTransformer Inference Arithmetic - Kipply\nTransformer Math 101 - EleutherAI\nTransformers from Scratch - Brandon Rohrer\nGPT in 60 lines of Numpy\n\nYouTube Videos §\n\nAttention is All You Need - Yannic Kilcher\nBuilding GPT from scratch - Andrej Karpathy\nIllustrated Guide to Transformers - Michael Phi\n\nCourses §\n\nTransformers United - Stanford CS25\nNLP with Deep Learning - Stanford CS224N\n\nDetailed understanding of the problem is in the transformerdeep page."},"notes/backlog":{"title":"My backlog","links":[],"tags":["seed"],"content":"Reading Backlog §\nRead but not summarized: §\n\nIntelligence and Spirit by Reza Negarestani.\n\nCombines philosophy, mathematics, logic, and computer science\n\n\nFanged Noumena by Nick Land\n\nA collection of essays on philosophy, cybernetics, and accelerationism\n\n\nNeuroplasticity by Moheb Costandi\n\nA book on the brain’s ability to rewire itself\n\n\nSea People: The Puzzle of Polynesia by Christina Thompson\n\nA book on the history of Polynesia\n\n\nEvolutionary Psychology: The New Science of the Mind by David Buss\n\nA book on the evolutionary psychology of human behavior\n\n\n\nTo read: §\n\nSilicon dreams: Information, Man and Machine by Robert W. Lucky\n\nA book on the history of information theory and its impact on society\n\n\nIn a flight of starlings: the wonders of complex systems\nThe Origin of Consciousness in the Breakdown of the Bicameral Mind\nWhen they severed earth from sky by Elizabeth Wayland Barber\nThe Recursive Mind: The Origins of Human Language, Thought, and Civilization by Michael C. Corballis\nIntroduction of artificial life by Christoph Adami\n\nPaper Backlog §\n\nUnderstanding latency hiding on GPUs\n\nLearning topics §\n\nLapack\nCobordism\nCarnap’s Vision of Language\nMatrix Multiplication on CPU\n\nPaper Backlog §\n\nReplica Symmetry Breaking Condition Exposed by Random Matrix Calculation of Landscape Complexity\n\nInner world building §\n\nGöbekli Tepe\nEleusinian Mysteries\n"},"notes/batchnorm":{"title":"Batch Normalization","links":["notes/mathterms"],"tags":["evergreen"],"content":"Batch normalization is a technique used to normalize the inputs of each layer in such a way that they have a mean of zero and a standard deviation of one.\nThe algorithm is relatively simple to implement:\n\nValues of x over a mini-batch B={x1​,x2​,…,xm​}\nOutputs: {yi​=BNγ,β​(xi​)}\n\nWe get the following equations:\nμB​=m1​i=1∑m​xi​\nσB2​=m1​i=1∑m​(xi​−μB​)2\nx^i​=σB2​+ϵ​xi​−μB​​\nyi​=γx^i​+β\nWhere γ and β are learnable parameters that scale and shift the normalized value, respectively. ϵ is a small constant to avoid division by zero.\nThe reason γ and β are learnable is that the network can learn to undo the normalization if it is not beneficial for the task at hand. Usually we set γ=σB2​+ϵ​ and β=μB​. This helps in the backward pass.\nInner workings §\nSmoothness and Initialization\nIn the beginning, BatchNorm was introduced to solve the problem of internal covariate shift. This is the change in the distribution of network activations due to the change in the parameters of the previous layer. This makes training difficult as the network has to adapt to the new distribution of activations. This has been debunked by Santurkar et al. in their paper.\nThe real reason BatchNorm works is that it re-parameterizes the training process. It makes the significantly more smooth, thus ensuring the gradients are more stable and we can use a larger variety of learning rates. The main impact is the improvement in the Lipschitzness of a loss and gradients. The loss changes at a smaller rate and the magnitudes of the gradients are smaller too.\nSmoothening effects impact the performance of the training algorithm in a major way. In deep neural networks, the loss function\nis not only non-convex but also tends to have a large number of “kinks”, flat regions, and sharp minimas. This makes gradient descent–based training algorithms unstable, e.g., due to exploding or vanishing gradients, and thus highly sensitive to the choice of the learning rate and initializations. BatchNorm reparametrization helps in making sure the direction of a computed gradient is a fair estimate of the true gradient direction. This stops the optimizer to take larger steps without the danger of sudden change in loss landscape. With this property, we can safely assume this makes the training significantly faster and less sensitive to hyperparameter choices.\nLet us assume you were training a model to predict if a mouse has cancer. We use the height and age as a predictor variables. We know the height of a mouse has lower variance than the age. If we don’t normalize the input, the model will learn to give more importance to the age than the height.\n\n    \n    \n\nOn the left, we can see the landscape before batchnorm. On the right side, we can see the landscape after batchnorm. The landscape is much smoother and the gradients are more stable.\nIs this smoothening effect a unique feature of BatchNorm? §\nThe authors of the paper Santurkar et al. have shown that the smoothening effect is not unique to BatchNorm. The authors observe that lp​ regularization performs better than batchnorm. The reason is lp​ regularization lead to larger distributional shifs and yield improved performance.\nReferences §\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\nHow Does Batch Normalization Help Optimization?\n"},"notes/brottl":{"title":"Brotli Compression","links":[],"tags":[],"content":"Parts of the paper §\n\nThis choice is motivated by the fact that the distribution of the frequency of a symbol is strongly influenced by its kind (whether it is a length, a distance, or a literal), its context (i.e., its neighboring symbols), and its position in the text. Briefly (since we will give further details in the next sections), each meta-block stores in the header the collection of Huffman codes that are used to compress the commands contained in its data part.\n\nlength, distance, and literal.\n\nFor example, if the files to be compressed have mixed content (such as HTML pages, JSON snippets, etc.), then block boundaries might mark the beginning and the end of the different homogeneous parts, and clustering could group together file parts that express homogeneous content spread all around these files.\n\n\nFor lengths, all values laying in blocks with the same block id are encoded using the same huffman code. For distances and literals, the choice of the huffman code to use does not depend solely on the block id but also on their context, so each cluster of blocks employs not just one huffman code but a set of huffman codes of canonical type,\n\n\nsubstrings that are equal except for small differences such as substitution, insertion, or deletion of few characters\n\n\nBrotli then deploys relative (copy) distances which express the delta between the current copy- distance and the previous copy-distance in the compressed stream. In this way a run of slightly different distances in the compressed stream is changed into a run of relative distances, which should take less compressed space.\n\n\nshort copies do typically occur close to the current position to be matched, whereas long copies are typically found farther in the input text\n\n\nThis implies that small distances have higher frequencies than long distances, but the latter typically refer to longer copies than the former. In terms of space occupancy, this means that far&amp;long copies are able to amortize the bit cost incurred by long distances because they squeeze long phrases\n\nIn summary, any reference to the dictionary is represented as a triplet that\nspecifies the length of the copy (between 4 and 24), the index of the dictionary word\n(according to the previously mentioned escape values), and the index of the word\ntransformation to be applied."},"notes/codechunks":{"title":"Code chunks","links":[],"tags":["sapling"],"content":"UMT code snippet §\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Set the random seed for reproducibility\ntorch.manual_seed(42)\n\n# Generate sample data from a cubic polynomial\nx = torch.linspace(-10, 10, 400).unsqueeze(1)  # Reshape x to have 1 column\ny = x**3 - 6*x**2 + x + 10  # Example cubic function\n\n# Split the data into training and testing sets\nx_train, x_test, y_train, y_test = train_test_split(x.numpy(), y.numpy(), test_size=0.2, random_state=42)\nx_train, x_test, y_train, y_test = map(torch.tensor, (x_train, x_test, y_train, y_test))\n\n# Convert to tensor datasets and create data loaders\ntrain_dataset = TensorDataset(x_train, y_train)\ntest_dataset = TensorDataset(x_test, y_test)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\n# Define the neural network model\nclass NeuralNet(nn.Module):\n    def __init__(self,nlayer=10):\n        super(NeuralNet, self).__init__()\n        self.layer1 = nn.Linear(1, nlayer)  # Hidden layer\n        self.relu = nn.ReLU()           # ReLU activation\n        self.layer2 = nn.Linear(nlayer, 1)  # Output layer\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.layer2(x)\n        return x\n\nresults = []\nfor size in range(1,1000):\n    model = NeuralNet(size)\n\n    # Loss and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n\n    # Training the model\n    num_epochs = 10\n    for epoch in range(num_epochs):\n        for inputs, targets in train_loader:\n            optimizer.zero_grad()  # Clear gradients for the next train\n            outputs = model(inputs.float())  # Forward pass\n            loss = criterion(outputs, targets.float())  # Compute the loss\n            loss.backward()  # Backward pass\n            optimizer.step()  # Optimize the weights\n\n         # Optionally print the loss every 10 epochs\n        if (epoch+1) % 10 == 0:\n            print(f&#039;Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}&#039;)\n\n    # Testing the model\n    model.eval()  # Evaluation mode\n    with torch.no_grad():\n        total_loss = 0\n        for inputs, targets in test_loader:\n            outputs = model(inputs.float())\n            loss = criterion(outputs, targets.float())\n            total_loss += loss.item()\n        if size%100==0:\n            print(f&#039;Mean Squared Error on Test Set: {total_loss / len(test_loader):.4f}&#039;)\n    results.append(model(x).squeeze().detach().numpy())\n    ```\n\n## MNIST Model code \n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\ngelu = nn.GELU()\nclass MNISTNet(nn.Module):\ndef init(self):\nsuper(MNISTNet, self).init()\nself.conv1 = nn.Conv2d(1, 15, kernel_size=5)\nself.conv2 = nn.Conv2d(15, 20, kernel_size=5)\nself.conv2_drop = nn.Dropout2d()\nself.fc1 = nn.Linear(320, 100)\nself.fc1_drop = nn.Dropout()\nself.fc2 = nn.Linear(100, 10)\ndef forward(self, x):\n  # First Loop\n  x = self.conv1(x)\n  x = F.max_pool2d(x, 2)\n  x = gelu(x)\n\n  # Second Loop\n  x = self.conv2(x)\n  x = F.max_pool2d(x, 2)\n  x = gelu(x)\n  \n  x = self.conv2_drop(x)\n\n  # Flatten\n  x = x.view(-1, 320)\n\n  #MLP \n  x = self.fc1(x)\n  x = gelu(x)\n  x = self.fc1_drop(x)\n\n  # second layer\n  x = self.fc2(x)\n  return F.log_softmax(x, dim=1)\n\n"},"notes/codesearch":{"title":"Program Synthesis","links":[],"tags":[],"content":"Program Synthesis is the task of automatically finding programs from the underlying programming language that satisfy user intent expressed in some form of constraints. This is very different from the traditional typical compiler which takes in high-level code and converts it to machine code. In program synthesis, We usually search in the space of programs to find the one that satisfies the constraints. This is a very active area of research and has many applications in software engineering, programming languages, and AI.\nProgram synthesis is a ridiculously hard problem. In its most general case ( Turing complete programming language), the possible space is all possible programs that can be computed. Even with a small number of constraints, the search space can grow exponentially. User Intent is another hard sub-problem to solve. The specification of the program can be ambiguous, incomplete, or even incorrect. The user might not know exactly what they want, or they might not be able to express it in a way that the synthesizer can understand.\nParts of Program Synthesis §\nA synthesizer is typically characterized by three key dimensions: the kind of constraints that it accepts as expression of user intent, the space of programs over which it searches, and the search technique it employs. Lets define these three dimensions:\nUser Intent §\nA logical specification is a logical relation between inputs and outputs of a program. Some users can express their needs through traces, examples, psudocode, or natural language. The synthesizer should be able to understand these constraints and search for a program that satisfies them.\nProgram Space §\nThe program space defines the set of possible programs that the synthesizer considers. This space is typically defined by a domain-specific language (DSL) or a subset of a general-purpose programming language. The choice of program space significantly impacts the efficiency and effectiveness of the synthesis process.\nKey aspects of the program space include:\n\nSyntax: The allowed structures and constructs in the language\nSemantics: The meaning and behavior of the language constructs\nExpressiveness: The range of programs that can be represented\nSize: The number of possible programs in the space\n\nSearch Technique §\nThe search technique is the algorithm used to explore the program space and find a program that satisfies the user’s intent. Common search techniques include:\n\nEnumerative search: Systematically generate and test candidate programs\nConstraint solving: Convert the synthesis problem into a logical constraint satisfaction problem\nVersion space algebra: Maintain and refine a compact representation of the space of consistent programs\nNeural-guided search: Use machine learning models to guide the search process\nDeductive synthesis: Apply logical inference rules to derive a program from the specification\n\nThe choice of search technique depends on factors such as the size and structure of the program space, the nature of the user intent specification, and the desired performance characteristics of the synthesizer.\nPapers to focus on: §\n\nProgram synthsis\n\nPersonally a good read to get started. Not to technical, but the basics are there\n\n\nDreamCoder\n\nMust read, starts with wake and sleep learning algorithms and goes into rough details\n\n\nStitch\n\nFigures out some shortcoming of DreamCoder and adds more step to increase the speed of synthesis\nGithub Repo has really good documentation and implementation (RU). Focus on the abstraction part of program synthesis.\n\n\nAbstract Beam\n\nHas a method to improve tradtional search which involves creating abstractions and then searching in the abstraction space.\n\n\n\nAlthough these papers are great, the number of papers compared to other areas of ML are very low.\nI would recommend focusing on Math based papers especially the ones which work with LEAN\n\nGPT-f\nFormal mathematics statement continious learning\nDeepseek prover 1/1.5\nAlphageometry\nAutoformalization\nBaldur\n\nPersonal opinion: program synthesis is a more of a engineering problem than a research problem. You have to rely more on the search part of symbolic execution. Be very doubtfull of papers whose results look abnormally great, most of the times the sheer inference compute is not shared\nRandom stuff i shared in the past:\nToo lazy to write a proper list, but here are some papers that i found interesting:\n\nhttps://arxiv.org/pdf/2308.03109\nhttps://arxiv.org/pdf/2402.14658\nhttps://arxiv.org/pdf/2401.03003\nhttps://arxiv.org/pdf/2405.15793\n"},"notes/compression":{"title":"Compression","links":[],"tags":["sapling"],"content":"\nMy 🥰 favorite part of information theory 🥰\n\nThe main engineering goal of compression is to represent a given any sequence xi​,x2​,…,xn​ with fewer bits than the original sequence. The main idea is to remove redundancy in the data. Reducing the number of bits is generally impossible, unless the source imposes certain constraints on the data.\nA simple example of this is the english language. If we see two different novels, model the distribution of the alphabet in both novels, we can see that the distribution of the alphabet is similar in both novels. This is because the english language has a certain structure and the distribution of the alphabet is similar in most cases. The goal of compression is to exploit this “similarity”.\nThere are three main types of compress for a random variable X:\n\nLossy compression: The original data can be perfectly reconstructed from the compressed data. X→W→X^, this would imply E[(X−X^)2]≤ distortion. Distortion can be thought as the error in the reconstruction.\nLossless compression: The original data can be perfectly reconstructed from the compressed data. X→W→X^, this would imply X=X^.\nNear-lossless compression: The original data can be approximately reconstructed from the compressed data. X→W→X^, this would imply E[(X−X^)2]≤ϵ.\n\nFixed Length almost lossless compression §"},"notes/conditionnumber":{"title":"Condition Number","links":["notes/tangentkernel"],"tags":["sapling"],"content":"Gradient converges exponentially fast to a the global minimum with the rate controlled by the condition number of the function.\nThe condition number (kf​) is the ratio of the largest  eigenvalue of the Hessian matrix(H) and the smallest eigenvalue of the tangent kernel (K) of the function.\nkf​=λmin​Kλmax​H​\nIn an ideal world, we would require the λmin​K to be very large and λmax​H to be very small. This would mean that the function is very well conditioned and the gradient descent converges very fast.\nIf the condition number is large, the function is ill-conditioned and the convergence of gradient descent is slow.\nThe condition number of a matrix, especially in the context of numerical linear algebra, is a measure of how sensitive the solution of a system of linear equations is to errors in the data or perturbations. It is also used more generally to describe the sensitivity of a function to its inputs. The condition number is influenced by multiple factors:\n\nScale of the Matrix: The scale of the numbers in the matrix can affect the condition number. If the matrix is poorly scaled (i.e., it has very large or very small numbers), it can have a high condition number.\nOrthogonality of Vectors: If the vectors in the matrix are close to orthogonal, the matrix tends to have a lower condition number. Matrices with vectors that are nearly linearly dependent will have higher condition numbers.\nMatrix Norm: The condition number is defined in terms of a matrix norm. Changes in the matrix norm (induced by changes in the matrix entries) can lead to changes in the condition number.\n\nTo make the condition number smaller and improve numerical stability, one can:\n\nRescale the Matrix: Pre-multiplying by a diagonal matrix with appropriate scaling factors can improve the condition number. This is known as equilibration.\nUse Singular Value Decomposition (SVD): SVD can be used to regularize a problem or to compute a more numerically stable solution.\nRegularization Techniques: In problems like linear regression, regularization methods like Ridge Regression or Lasso Regression can improve the condition number by adding a penalty term that discourages large coefficients.\nIncrease Precision: Using a higher precision for numerical computations can sometimes mitigate the effects of a high condition number, but it doesn’t reduce the condition number itself.\nPerturbation: Small changes to the matrix (perturbation) can sometimes reduce the condition number, but this method needs to be applied carefully as it changes the original problem.\nRow or Column Operations: Sometimes, performing certain row or column operations (like in Gaussian elimination) can lead to a matrix with a smaller condition number.\nPreconditioning: Before solving a linear system, applying a preconditioner can transform the system into one that has a better condition number.\n\nIt’s important to note that there is often a trade-off between improving the condition number and maintaining the accuracy or integrity of the original problem. The appropriate technique depends on the specific context and requirements of the application at hand."},"notes/curves":{"title":"Curves in Machine Learning","links":[],"tags":["sapling"],"content":"Precision and Recall §\nRisk and Complexity §\nVapnik–Chervonenkis dimension §\nBias-Variance Tradeoff §"},"notes/distanemetrics":{"title":"Distance Metrics","links":[],"tags":["sapling"],"content":"Hellinger Distance §\nThe Hellinger distance is a measure of the similarity between two probability distributions. It is defined as:\nH2(P,Q)=21​∑i=1n​(pi​​−qi​​)2\nwhere P and Q are two probability distributions over the same set of n events.\nIn the continuous case, the Hellinger distance between two probability density functions p(x) and q(x) is defined as:\nH2(p,q)=21​∫(p(x)​−q(x)​)2dx=1−∫p(x)q(x)​dx\nMahalanobis Distance §\nimport numpy as np \nimport pandas as pd  \nimport scipy as stats \n  \n# calculateMahalanobis function to calculate \n# the Mahalanobis distance \ndef calculateMahalanobis(y=None, data=None, cov=None): \n  \n    y_mu = y - np.mean(data) \n    if not cov: \n        cov = np.cov(data.values.T) \n    inv_covmat = np.linalg.inv(cov) \n    left = np.dot(y_mu, inv_covmat) \n    mahal = np.dot(left, y_mu.T) \n    return mahal.diagonal() \n"},"notes/distribution":{"title":"Distributions summarized","links":[],"tags":["evergreen"],"content":"Bernoulli distribution §\nThe Bernoulli distribution is a discrete probability distribution for a random variable which takes the value 1 with probability p and the value 0 with probability q=1−p.\nf(k;p)=pk(1−p)1−k for k∈{0,1}\nWhich is basiically, at k=1 (success) probability is p and at k=0(failure) probability is 1−p.\nMean §\nThe mean of the Bernoulli distribution is p.\nSimple proof:\nE[X]=∑k=01​k⋅f(k;p)=0⋅(1−p)+1⋅(1⋅p=p)=p\nVariance §\nThe variance of the Bernoulli distribution is p(1−p).\nSimple proof:\nE[X2]=∑k=01​k2⋅f(k;p)=02⋅(1−p)+12⋅p=p\nVar[X]=E[X2]−E[X]2=p−p2=p(1−p)\nBivariate Gaussian distribution §\nHave to add the formulae and the properties of the bivariate gaussian distribution §"},"notes/fetesize":{"title":"Fetisization of the prime intellect","links":[],"tags":[],"content":"\nsuperficial blurring the lines between intution and intellect, among us and machine, between the real and the virtual.\n\nThis work is inspired by works of Sarvepalli Radhakrishnan, François Chollet, Stanford Encyclopedia of Philosophy, works of Reza Negarestani and my inner conflict of religion and scientific rationality.\nInterplay between the realized and realizor §\nWill add here later\nBeyond superficial pattern matching §\nSorry AI bros, i switched sides\nWill add here later\nLanguage and logic as a living organism §\nWill add here later\nTechnology and its role in time asymmetry §\nWill add here later\nConception and Transformation §\nWill add here later\nStaring into the abyss §\nWill add here later\nBreakdown of minds, Schizophrenia and dreams §\nWill add here later\nContemplation as a cultivation of intelligence and divine will §\nWill add here later\nReferences §\n\nOn intution and discursive reasoning in aristotle\n\nWork in progress, I have always wanted to write about this but never got around to it. This is diary entry not a debate. §"},"notes/ff":{"title":"Neural networks","links":["notes/codechunks","notes/batchnorm"],"tags":["sapling"],"content":"Deep Neural Networks (DNNs) form the backbone of modern machine learning. One of the strength of DNNs is its high expressivity power. This capability to capture any flexible data representation allows deep neural networks to have widespread use from biology or weather prediction.\nUniversal Approximation Theorem §\nUniversal Approximation Theorem(UMT) states that any feedforward networks with a linear output layer, one hidden layer, activation function t can appoximate any continuous functions on a subset of Rn\nAlthough a Feedforward Neural Network with a single layer can represent any function, the width has to be infinite. The UMT doesn’t guarantee whether the model can be learned or generalized properly.\nProof §\nA neural network C can model any function given\na sample size n and d dimensions if: For every sample set S∈Rd and ∣S∣=n and every function defined on this sample set f:S−&gt;R, we can find a set of weights such that C(x)=f(x)\nLet us prove the simple case:\n\nThere exists a two-layer neural network with ReLU activations and 2n+d weights that can represent any function on a sample of size n in d dimensions.\n\nFirst we would like to construct a two-layer neural network C:Rd→R. The input is a d-dimensional vector, x∈Rd. The hidden layer has h hidden units, associated with a weight matrix W∈Rd×h, a bias vector −b∈Rh and ReLU activation function. The second layer outputs a scalar value with weight vector v∈Rh and zero biases.\nThe output of network C for an input vector x can be represented as follows:\nC(x)=v⊤max{Wx−b,0}T=∑i=1h​vi​max{W(:,i)​x−bi​,0}\nwhere W(:,i)​ is the i-th column in the d×h matrix.\nGiven a sample set S={x1​,...,xn​}T and target values y={y1​,...,yn​}, we would like to find proper weights W∈Rd×h, b,v∈Rh so that C(xi​)=yi​, ∀i=1,...,n.\nLet’s combine all sample points into one batch as one input matrix X∈Rn×d. If set h=n, XW−b would be a square matrix of size n×n.\nMReLU​=max{XW−b,0}=​max{x1​W−b}...max{xn​W−b}​​=[xi​W(:,j)​−bj​∣xi​,j​]\nWe can simplify W to have the same column vectors across all the columns:\nW(:,j)​=w∈Rd,∀j=1,...,n\nLet ai​=xi​w, we would like to find a suitable w and b such that\nb1​&lt;a1​&lt;b2​&lt;a2​&lt;⋅⋅⋅&lt;bn​&lt;an​.\nThis is always achievable because we try to solve n+d unknown variables with n constraints and xi​ are independent. Set random xi​, sort the ak​ values. In between each ak​ set a bk​ value. Then MReLU​ becomes a lower triangular matrix, due to the above condition.\nMReLU​=[ai​−bj​]i×j​=​a1​−b1​⋮ai​−b1​⋮an​−b1​​0⋱…an​−b2​​0ai​−bi​…​……⋱…​0⋮0⋮an​−bn​​​\nIt is a nonsingular square matrix as det(MReLU​)=0, so we can always find suitable v to solve vMReLU​=y (In other words, the column space of MReLU​ is all of Rn and we can find a linear combination of column vectors to obtain any y).\nYou can also see the experimental results at UMT Proof\nWhy NN can even model noise? §\nSince, we now know any NN are universal approximaters. We can safely assume that they are also able to learn unstructed noice perfectly.\nIf labels of image classification dataset are randomly shuffled, the university principle of NN can still acheive net zero training loss. This doesn’t change with any degree of regularization.\nRegularization is a not always the right answer §\nRegularization is a common technique to prevent overfitting. However, it is works very differently in the context of neural networks. In contrast to traditional methods, regularization in neural networks acts as a tuning paramter that helps improve the test error. No single regularization seems to be critical independent of other terms. Thus, it is unlikely that regularizers are the fundamental reason for generalization.\n\n    \n\nJust move bro §\nConsidering a neural network with a great number of parameters, forming a high-dimensional parameter space, when learning happens on this high-dimensional objective landscape. The shape of the parameter space manifold is critical. For example, from the Batch Normalizations page, we can clearly see that a smoother manifold is benficial for optimization. It allows us to have more predictive gradients and larger learning rates.\nEven though the parameter space is huge, fortunately we don’t have to worry too much about the optimization process getting stuck in local optima, as it has been shown that local optimal points in the objective landscape almost always lay in saddle-points rather than valleys. In other words, there is always a some dimensions containing paths to leave local optima and keep on exploring. As they say Just move bro.\nThere is some mindblowing math here specifically related to saddle points. Just to summarize:\n\nCritical points concentrate along a monotonically increasing curve in the ϵ−α plane.\nEigenvalues do not seem to be exactly distributed according to the semicircular law, their distribution does shift to the left as the error increases\nA plateau around any critical point of the error function of a neural network.\nA trust region approach is a practical way to escape saddle points. This is done by making all the eigenvalues of the Hessian positive  by adding a term α to the diagonal of the Hessian matrix. Rescale the gradient by the inverse of the modified eigenvalyes ( decrease the step size in general). To ensure descent along every eigen-directions, one must increase α until all the eigenvalues are positive.\n\n\nDeep networks perform\nClassic difference between NN and traditional ML §\n\n    \n\nHeterogeneous Layer Robustness §\nLottery ticket §\nNeural tangent kernel §\nInfinite width networks §\nDeterministic NTK §\nLinearized NTK §\nLazy training §\nReferences: §\nSource: 1 1/2 lessons of deep learning"},"notes/geometrymeaning":{"title":"Geometry of Meaning by Peter Gardenfors","links":[],"tags":["sapling"],"content":"\n    \n\nIn this book, we will be covering the main points of the book. We will be drawing parallels between the cognitive interpretation and computational representation of language models. My aim here is to better understand representation learning in language models but from a cognitive science perspective.\n1. Semantics as a meeting of minds §\n\n“Langue (language as a social system of signs) and parole (the individual speech acts).”\n“The signs of language are “collective products of social interaction, essential instruments through which human beings constitute and articulate their world,”\nLanguage games as an act of communication.\n“philosophy of language, one can find two fundamentally different answers to the ontological question, one realistic and one cognitive”\n“ A special aspect of  the semantic question is how meanings of composed expressions relate to the meanings of the constituents. ”\nLanguage learning is sample efficent. Humans learn about 10 words per day while growing up\nGrounding of language( spatial repesentation of words) among young people is not grounded in ambigious situations like adults but in simple use cases.\n“Early word learning often takes place in the context of infants’ active exploration of objects: infants do not simply look passively at the jumble of toys on the floor but rather use their body—head, hands, and eyes—to select and potentially visually isolate objects of interest, thereby reducing ambiguity at the sensory level.”\n\nSemantic Theory basic foundations: §\n\nThe ontological criterion\n\nA theory of semantics shall explain what kind of entities meanings are.\n\n\nThe semantic criterion:\n\nA theory of semantics shall account for the relation between communicative expressions and their meanings\n\n\nThe epistemological criterion:\n\nA theory of semantics shall explain how the meanings of communicative acts can be learned”\n\n\nThe social criterion:\n\nA semantic theory shall account for the relation between the meaning systems of individual speakers and their languages\n\n\nA semantic theory shall account for the relation between perceptual processes and meaning.\nA semantic theory shall account for the relation between actions and meaning.\n\n\nMost of the analyses within cognitive linguistics concern relations between words and representations of concepts.\n\n\nEvery word, possibly except for syntactic markers, is supposed to correspond to an image schema\n\n2. Conceptual Spaces §\nVery similar to representation learning in DL, Topology and geometry allow us to talk about nearness and distance in a space.\nThe cognitive interpretation concerns the structure of human perception and their inner worlds.\nThe scientific interpretation, on the other hand, deals with how different dimensions are presented within a scientific theory. These are different representations and need to be treated as such\nHuman communication §\n\nHuman communication involves not just an understanding of the meaning of a sign itself but an understanding that a sign can be used communicatively.\nThe communicative sign function implies an understanding that the sign has the same meaning for the addressee as for the sender. But “having the same meaning” is a reflexive notion, and this implies at least some degree of third-order intersubjectivity\n\n\n“a semantic theory based on meetings of minds will be presented. According to this view, the meanings of expressions do not reside in the world or (solely) in the image schemas of individual users but emerge from the communicative interactions of language users.” - Book\n\n\nThere are two basic types of meetings of minds:\n\nFast meeting\n\nminds concerns expressions that obtain their meaning during a communicative interaction\nMom pointing to a toy and the child looking at the toy learns what lego is\nPopularly called joint attention (basically the same in transformers multi-attention mechanism)\n\n\nSlow meeting\n\nThe slow  concerns how a community adjusts its uses of words, gestures, and so on, so that they obtain relatively fixed meanings within the community that are largely independent of any particular communicative context.\nLanguage traditional widely agreed on. ofc gyat, is a counter example\n\n\n\n\n\nThe flaw which the author sees is:\n\n“In my opinion, it is necessary to separate meaning relations that are based on similarity judgments from other types of relations. I suggest a narrower characterization of domain based on dimensionality and argue that many other aspects of meaning that have been sorted under the notion instead concern part-whole and other meronomic relations.”\n\nAKA, partition space according to relational definitions only not to be recursively higher.\nThis can be explained as dimensional and meronomic difference. In smooth brain terms, representaion bad in hierarchical terms. Dimension flatness\nWe can fix representaion bad in hierarchical terms. These processes are tedious and as of march 2024, i have not seen any LM do this well. Some possible academic solutions include:\n\n\nPoincaré Embeddings\n\nMap embeddings to piocaré space. Top points neat the center, go down the tree, place away from the center. Use\nRiemannian optimization to optimize the embeddings.\n\n \n  \n  \n  \n\nExample of Poincare ball, all the subspaces are of same size\n\n\n\nSubspace-based approach\n\nPartition spaces into blocks.\nAdd conditions for each member to be in the block.\nAdd a subspace indicator function to model membership.\nAdd vectors quantifies to what extent the i-th token vector belongs to the j-th token vector ( Basically self attention).\nSubspace with different degrees of precision one for spatial, once inside spatial, Voronoi space of it.\nThis is my understanding of it, i frankly think this approach has major flaws especially related to handling positions and same word different meanings situation. Very confident this doesn’t scale\n\n\n\nMetaphors and Metonymies §\nMetaphor (drawing a similarity between two things) and metonymy (drawing a contiguity between two things) are two fundamental opposite poles along which a discourse with human language is developed. These mappings are basicallys two poles:\n\nsimilarity and contiguity\ncondensation and displacement\nparadigmatic and syntagmatic poles\n\nIt has been argued that the two poles of similarity and contiguity are fundamental ones along which the human mind is structured; in the study of human language the two poles have been called metaphor and metonymy, while in the study of the unconscious they have been called condensation and displacement. In linguistics, they are connected to the paradigmatic and syntagmatic poles - wiki\nCurrent Language models are very sensitive to a metaphor’s novelty. GPT-4 doen’t have this ability fully.\n\nThe ability to generate interpretations of novel metaphors must not be confused with the ability to generate novel metaphors. Besides being able to generate interpretations of metaphors, as shown here, LLMs can certainly generate texts in which metaphors appear. However, to the best of our current knowledge, the metaphors an LLM might generate are limited to those that human writers have already formed and planted into texts, thereby making humanity’s store of metaphors available to be mined by LLMs\n-Nicholas,Dušan Stamenkovićb, Keith in Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors\n\nSupport §\nCognitive interpretation and computational representation example with color §\n\n\n\n&gt; Cognitive representation of colour \n\n\n\n&gt; Computational representation of colour\nVoronoi diagram §\nIn mathematics, a Voronoi diagram is a partition of a plane into regions(tessellation) close to each of a given set of objects.  In the simplest case, these objects are just finitely many points in the plane. For each seed there is a corresponding region, called a Voronoi cell, consisting of all points of the plane closer to that seed than to any other.\nExtensibly used in FAISS and other ANN search algorithms\nExample of a varonoi diagram:\n\n\n\n\nComputational representation of colour\n\nThe edjes are infinite lines here. Assume top left point is (0,0). Then the point are:\n\n(∞,∞) is in the dark blue domain\n(∞,−∞) is in the light blue domain\n(−∞,∞) is in the grey domain\n(−∞,−∞) is in the blue domain\n\nA Voronoi tessellation depends on the level of specificity of the class of concepts represented by the tessellation. (Density of points)\nAuthor argues humans do something same. We partition the world into regions of concepts.\nMetonymy §\nMetonymy’s is a  figure of speech in which the name of an object or concept is replaced with a word closely related to or suggested by the original\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer(&quot;all-mpnet-base-v2 &quot;)\n\ncosine_distance(model.encode([&quot;crown&quot;,&quot;king&quot;]))\n0.482790470123291\n\ncosine_distance(model.encode([&quot;white house&quot;,&quot;president&quot;]))\n0.44765037298202515\n\n\nMainstream cognitive semantics does not provide a viable account of the social criterion. This boils down to LM’s inability to account for the social dimension of meaning, which is a crucial aspect of language use."},"notes/gpt2code":{"title":"GPT2 Implementation","links":[],"tags":[],"content":"Layer Norm Code §\nclass LayerNorm(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.w = nn.Parameter(t.ones(cfg.d_model))\n        self.b = nn.Parameter(t.zeros(cfg.d_model))\n\n    def forward(self, residual):\n        # residual: [batch, position, d_model]\n        # output: [batch, position, d_model]\n        mean = residual.mean(dim=-1, keepdim=True)\n        variance = residual.var(dim=-1, keepdim=True,correction=0) + self.cfg.layer_norm_eps\n        \n        residual = (residual-mean)/(variance**0.5)\n        return residual*self.w  + self.b\n\nEmbedding §\nclass Embed(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n\n    def forward(self, tokens):\n        # tokens: [batch, position]\n        # output: [batch, position, d_model]\n        return self.W_E[tokens]\n\nPositional embedding §\nclass PosEmbed(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n\n    def forward(self, tokens):\n        # tokens: [batch, position]\n        # output: [batch, position, d_model]\n        batch, seq_len = tokens.shape\n        return einops.repeat(self.W_pos[:seq_len], &quot;seq d_model -&gt; batch seq d_model&quot;, batch=batch)\n\nSelf-Attention §\nAttention is a tricky code block. We will be using einsum to make our calculations easier.\nclass Attention(nn.Module):\n    def __init__(self, cfg: Config):\n        super().__init__()\n        self.cfg = cfg\n        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n        self.scale = cfg.d_head**0.5\n        self.softmaxi = nn.Softmax(dim=-1)\n        self.register_buffer(&quot;IGNORE&quot;, t.tensor(-1e5, dtype=t.float32, device=&quot;cuda&quot;))\n\n    def forward(self, normalized_resid_pre: t.Tensor):\n        # normalized_resid_pre: [batch, position, d_model]\n        # output: [batch, position, d_model]\n\n        # Calculate query, key and value vectors\n        ## Get the query matrix\n        query_mat = einsum(&quot;batch position_q d_model, n_heads d_model d_head -&gt; batch position_q n_heads d_head&quot;, normalized_resid_pre, self.W_Q) + self.b_Q \n        key_mat = einsum(&quot;batch position_k d_model, n_heads d_model d_head -&gt; batch position_k n_heads d_head&quot;, normalized_resid_pre, self.W_K) + self.b_K\n        val_mat = einsum(&quot;batch position_v d_model, n_heads d_model d_head -&gt; batch position_v n_heads d_head&quot;, normalized_resid_pre, self.W_V) + self.b_V\n        \n        # Calculate the attention scores \n        \n        atten_qk = einsum(&quot;batch position_q n_heads d_head , batch position_k n_heads d_head -&gt; batch n_heads position_q position_k &quot;, query_mat, key_mat) \n        atten = (self.apply_causal_mask(atten_qk/self.scale)).softmax(-1)\n        \n        val_mat_res = einsum(&quot;batch position_v n_heads d_head , batch n_heads position_q position_v -&gt; batch position_q n_heads d_head&quot;,  val_mat,atten)\n        \n        attn_out = einsum(&quot;batch position_q n_heads d_head,  n_heads d_head d_model -&gt; batch position_q d_model&quot;, val_mat_res, self.W_O) + self.b_O\n\n        return attn_out\n\n    def apply_causal_mask(self, attn_scores: t.Tensor):\n        # attn_scores: [batch, n_heads, query_pos, key_pos]\n        # output: [batch, n_heads, query_pos, key_pos]\n\n        # Define a mask that is True for all positions we want to set probabilities to zero for\n        mask = t.triu(t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n        # Apply the mask to attention scores, then return the masked scores\n        attn_scores.masked_fill_(mask, self.IGNORE)\n        return attn_scores\n\n\n\nMLP §\nclass MLP(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n\n    def forward(self, normalized_resid_mid):\n        # normalized_resid_mid: [batch, position, d_model]\n        # output: [batch, position, d_model]\n        ll1 = einsum(&quot;batch position d_model , d_model d_mlp -&gt; batch position d_mlp&quot;,normalized_resid_mid,self.W_in) + self.b_in\n        act1 = gelu_new(ll1)\n        ll2 = einsum(&quot;batch position d_mlp , d_mlp d_model -&gt; batch position d_model&quot;,act1,self.W_out) + self.b_out\n        return ll2\n"},"notes/index":{"title":"Notes","links":[],"tags":[],"content":"All of my notes live here! Still very much a work in progress!"},"notes/informationarch":{"title":"Information Indexing","links":[],"tags":[],"content":"\nLooking for good content in a world where machines can mass produce content\n\nIn this page, i have some disparate notes on information indexing and how smarter LMs break the traditional search engine paradigm. I have recently been puzzled how would search look in a world where information is vast but attention is not. Recent advancements in LLMs have made me question the traditional search engine paradigm can even survive with the sparsity of concentration. We have to think what is means to search and who/what are we searching. This post will go through how would an ideal search system look like regardless of the consumer, what makes the search system special for us and then finally how would you model a search system that is\nAs someone who works in item search, it is seemingly trivial when you have a fixed index and safeguards in place to ensure quality. However, the world is not so simple. The web is a hivemind of rouge ai’s and cognitive human thoughts wrapped with exabytes of data waiting to be indexed.\nDive deeper into the world of information retrieval, plung into the depths of the web, drop your anchors of abilities limited by reality and experience life through artificiality of the web.\n\n\nObligatory cyberpunk netrunner image\n\nArbitrator of truth §\nRelevance is defined as when an object (link, website, item and truth) is thought as relevant to the needs of the user. Thus the arbitrator of truth is the user. The user is the one who decides what is relevant and what is not. This opens a pandora’s box of oppurtunities and problems, as the user is not a single entity but a collective of individuals with different needs and wants.\nEven though two user can submit the same query, the assesments of the truth can be widely different. The relevance for each document can change when the user’s needs change. You can think of search being reflexive. Every query changes the user and the user changes the systems understanding of the query.\nHow would you model a structure of a space and what conditions would you impose to ensure the space represents this underlying structure?\nDivide and Conquer §\nLet us break the whole system into smaller parts. We can think of the system as a series of interconnected parts that work together. From the five axiom theory, we get the following axioms:\n\nDefinability:\n\nThe compilation of responses relevant to a topic can be delegated only to the extent to which the inquirer can define the topic in terms of concepts and concept relations.\n\n\nOrder:\n\nAny compilation of responses relevant to a topic is an order-creating process.\n\n\nSufficient degree of order:\n\nThe demands made on the degree of order increase as the size of the collection andlor the frequency of searches increases.\n\n\nRepresentational predictability:\n\nThe accuracy of any directed search for relevant texts (especially the recall ratio) depends on the predictability of the modes of expression for concepts and concept relations in the search file.\n\n\nRepresentational fidelity:\n\nThe accuracy of any directed search for relevant texts (especially the precision ratio) depends on the fidelity with which concepts and concept relations are expressed in the search file.\n\n\n\nReferences §\nThe Geometry of information retrieval\nThe Five Axiom Theory of Indexing and Information Supply"},"notes/inftheory":{"title":"Information theory basics","links":["notes/distribution","notes/compression"],"tags":["evergreen"],"content":"In this webpage, we will be talking about the basics behind information theory. Before you start, you need to have basic understanding of probability theory and distributions. This page is crucious to understand the basics of how any neural network can model any distribution.\nEntropy §\nEntropy is a measure of the uncertainty associated with a random variable. It is a measure of the average amount of information produced by a stochastic source of data. The concept of entropy was introduced by Claude Shannon in his 1948 paper “A Mathematical Theory of Communication”.\nH(X)=−∑x∈X​p(x)logP(x)\nwhere H(X) is the entropy of the random variable X and P(x) is the probability mass function of X.\nExample of Bernoulli distribution\nH(X)=−plogp−(1−p)log(1−p)\nNow why does this term mean entropy?\nThe name entropy comes from thermo-dynamics. In thermo-dynamics, entropy is a measure of the disorder of a system. In information theory, entropy is a measure of the uncertainty of a random variable.\nThe logarithm provides a lot of intresting combinations which map to how data is representing. The base of the logarithm is usually 2 but this can be changed to match any role. a base 2 uses bits, a base 10 uses dits, base 256 uses bytes and a base e uses nats (information content of an event when the probability is e1​ ).\nProperties §\n\n\nEntropy is always positive or zero. It is zero when the random variable is deterministic.\n\n\nEntropy is maximized when all outcomes are equally likely. This is because the entropy is a measure of uncertainty and when all outcomes are equally likely, the uncertainty is maximized.\n\n\nEntropy is minimized when one outcome is certain. This is because the entropy is a measure of uncertainty and when one outcome is certain, the uncertainty is minimized.\n\n\nEntropy is additive. This means that the entropy of the sum of two random variables is the sum of the entropies of the two random variables. H(X,Y)=H(X)+H(Y)\n\n\nPermutation invariance. The entropy of a random variable is invariant under permutations of the outcomes of the random variable. This means that the entropy of a random variable is the same as the entropy of the random variable with the outcomes permuted.\n\n\nSubadditivity. The entropy of the sum of two random variables is less than or equal to the sum of the entropies of the two random variables. H(X+Y)≤H(X)+H(Y)\n\n\nThis properties are nice, but what happens if i have two functions how do i measure the information content of the two functions?\nSimple explanation §\nLet us assume, you are a weather outpost and you have to send an message to the weather station.\n\n\nIf the message can be “sunny” or “rainy”, then the entropy is 1 bit.\n\n\nIf the message can be “sunny” or “partially sunny” or “partially rainy” or “rainy”, then the entropy is log2​(4) = 2\n\n\nLet is now take the examples where the sunny and rainy is a probability distribution:\n\nIf the message can be “sunny” with 0.75 probability and “rainy” with 0.25 probability. You get a message rainy, then the number of bits you require is 2 bits(1/4 possible outcomes). If you message as sunny, then you require 0.41 bits(3/4 possible outcomes). The average number of bits required is:\nWhen it is sunny = P(sunny)* Bits(of sunny) =  0.75∗0.41\nWhen it is rainy = P(rainy)* Bits(of rainy) =  0.75∗2\nAverage bits = 0.75∗0.41+0.25∗2=0.81 bits\nThis means if you stay in Doha the average number of bits required to send a message is lower as it is always sunny but in Seattle where the weather is unpredictable, you require more bits to send the message.\nThis is called Cross-Entropy\n\n\n\nIf the message “sunny” or “partially sunny” or “partially rainy” or “rainy” with probabilities 0.40, 0.30, 0.20, 0.10 respectively. Let us assume, you have telekenis. You can accuratly know the number of bits required. You calculate that 0.4log(0.4)+0.3log(0.3)+0.2log(0.2)+0.1log(0.1) = 1.84 bits. But you are computer, you have to represent it as binary so you send 2 bits (00 for sunny, 01, partially sunny, 10 for partially rainy and 11 for rainy). This is your cross-entropy. Therefore your telekenis is saving 0.16 bits. This is called Kullback-Leibler divergence.\n\n\nMutual Information §\nMutual information is a measure of the amount of information that one random variable contains about another random variable. It is a measure of the reduction in uncertainty of one random variable due to the knowledge of another random variable.\nGeometric meaning: “mutual information as distance to product distributions”\n\nYou and your wife got a message, you have to respond together. Your message was 2 bits long and your wife’s message was 3 bits long. sus. You notice that the message is similar. Being a nerd, you know that 0.75 bits is shared ( This is called mutual information). Now you both respond with 4.25 bits (This is the entropy). Both happy jpeg.\n\nSome properties of mutual information:\n\nSymmetry: I(X;Y)=I(Y;X)\nNon-negativity: I(X;Y)≥0\nThe amount of entropy reduction is the mutual information between the two random variables. H(Y∣X)−H(X)=I(X;Y)\n\nNow, we have two cases where the mutual information:\n\nPY∣X​→max(I(X;Y)) is the capacity of the set of distributions\nPX​→min(I(X;Y)) is the lossy compression\n\nNow the natural question is, how do we maximize the capacity? The key point here is the data mirrors the prediction, we have no noise. Maximizing the capacity. From this, we can naturally say that a medium where we are receiving hardly any noise and maximizing the information, we are maximizing the capacity.\nAnother intutive way to relate to ML is the difference between cross entropy to KL divergence. Cross entropy is the average number of bits required to send a message, KL divergence is the number of bits saved if you know the distribution.\nThe mutual information can be understood as the expectation of the KL divergence.\nData Processing Inequality §\nPost-processing cannot increase information\nData processing inequality is an information theoretic concept that states that the information content of a signal cannot be increased via a local physical operation. This can be reffered as the “intrinsic information” of the signal. It is not necessary, we are able to decipher the message, but the information content of the message is not changed. (Skill Issue)\nIn real life, we come across multiple examples of the opposite. Data processing helps us to decipher the message. So post processing can “increase” the information content, but this is an miss-understanding. The information content of the message is not changed, but you ability to decipher the message is increased(skill issue). This is a change in a relative basis, not an absolute basis.\nIf we have a basic markov chain, X→Y→Z, i.e Z and Y contain the same information about X.\nI(X;Y)⩾I(X;Z),\nA natural conclusion of this is noise inside a channel PY∣Z​ must reduce the information that Y carries about the data X, regardless of how smart the look up X → Y is.\nCapacity = information radius §\nThis is the maximum amount of information (usually measured in bits per second) that can be transmitted through a channel, given the limitations imposed by noise and other impairments. The formal definition of channel capacity C is given by the maximum mutual information between the input and output of the channel.\nC=maxPX​​I(X;Y)\nwhere PX​ is the input distribution of the channel and I(X;Y) is the mutual information between the input and output of the channel. In summary what is a distribution that maximizes the mutual information between the input and output of the channel.\nCapacity can be thought as the radius of the set of distributions where distances are measured in terms of divergence. Another way to think about this is the radius of the small divergence ball that encompasses all distribution.\nNow, one natural question comes is for what input we get the maximum mutual information. A input distribution Px​ that results in the maximum mutual information between the input and output of the channel is called the capacity achieving input distribution.\nSome real life examples are:\n\nBinary Symmetric Channel (BSC): For a binary symmetric channel with bit-flip probability p, the capacity-achieving input distribution is when each input bit (0 or 1) is equally likely (i.e., each has a probability of 0.5). This maximizes the mutual information between the input and the output.\nAdditive White Gaussian Noise (AWGN) Channel: For an AWGN channel, where the noise added to the signal is Gaussian, the capacity-achieving input distribution is a Gaussian distribution. The mean and variance of this distribution depend on the power constraints of the channel.\n\nShanon Limit §\nThe Shannon limit is the theoretical maximum rate at which data can be transmitted over a communication channel without error.\nC=Blog2​(1+NS​)\nwhere C is the channel capacity, B is the bandwidth of the channel, S is the signal power, and N is the noise power. This makes a lot of intutive sense, the more the bandwidth, the more the information that can be transmitted. The more the signal power, the more the “value” information that can be transmitted. The more the noise power, the less the “mutual” information that can be transmitted.\nProducts Channels and Sources §\nThe distinction between the behavior of mutual information in a system with a product channel and one with a product source reflects fundamentally different aspects of how information is processed and transmitted in information systems. To explore this, let’s differentiate the concepts and contextualize their implications in terms of information theory.\nProduct Channel §\nA product channel consists of multiple independent sub-channels where the input X can be split into separate components X1​,X2​,…,Xn​ that are transmitted independently through respective sub-channels to outputs Y1​,Y2​,…,Yn​. The key property here is that each sub-channel functions independently of the others, leading to the mutual information between the overall input and output being the sum of the mutual information of the individual pairs:\nI(X;Y)=I(X1​;Y1​)+I(X2​;Y2​)+⋯+I(Xn​;Yn​)\nThis setup maximizes the overall mutual information by maximizing each component independently, reflecting an optimal use of the channel capacity without interference.\nProduct Source §\nA product source, on the other hand, refers to a scenario where the source itself produces outputs that are statistically independent across its components. Here, X=(X1​,X2​,…,Xn​) is such that each component Xi​ is generated independently of the others. The key focus for a product source relates to how the source’s independence properties influence the design and choice of the channel for minimizing mutual information, particularly in contexts like privacy, security, or noise minimization.\nMutual Information Minimization in Product Source §\nWhen it comes to a product source, the goal might shift towards minimizing mutual information for various reasons, such as ensuring privacy or reducing predictability of the transmitted information from the observed output. The statement, “For a product source, the MI-minimizing channel is a product channel,” implies that:\n\n\nPreservation of Independence: Using a product channel maintains the independence of the components of the input in the output. Since each Xi​ is processed independently, the outputs Yi​ derived from them through the channel do not introduce any new dependencies among the components. This is crucial in scenarios where the addition of inter-component dependencies could inadvertently increase mutual information.\n\n\nChannel Design: If the objective is to minimize mutual information (perhaps to obscure the data or protect privacy), designing the channel such that it respects the independence of the source components and does not allow for cross-channel influences is vital. This could mean, for example, using noise addition or other transformations that treat each Xi​ independently, thus ensuring that any information leaked through the channel does not compound through interactions between different components.\n\n\nPractical Example §\nIn a security-sensitive communication system, suppose a device sends multiple types of data (e.g., location, temperature, user activity) separately. If privacy is a concern, designing the transmission channel to treat each type of data independently (product channel) ensures that an eavesdropper learning about one type of data (say, temperature) gains no information about the other types (like location or activity), thus minimizing the total mutual information leakage across these diverse data streams.\nIn summary, while the product channel maximizes mutual information by exploiting independence to enhance channel capacity, a product source paired with a product channel can be used strategically to minimize mutual information, thereby reducing the amount of information an eavesdropper can infer about the original source data from the channel output. This dual approach highlights the adaptability of information theory principles across different objectives—maximizing capacity or preserving privacy.\nEntropy rate §\nThe entropy rate is a concept in information theory that measures the average amount of information produced by a stochastic process per time unit. It is often used in the context of random processes or time series data, where the values are not identically distributed or there are dependencies between successive observations.\nFor a stochastic process Xt​ where t represents discrete time steps, the entropy rate H(X) is defined as the limit of the avergae entropy of the first n random variables divided by n as n approaches infinity:\nH(X)=limn→∞​n1​H(X1​,X2​,…,Xn​)\nThe entropy rate for vareity of processes:\n\nIndependent Identically Distributed (IID) Process: If Xi​ are IID, the entropy rate is the same as the entropy of a single random variable.\nMarkov Process: For a Markov process, where the next state depends only on the current state (not on the history), the entropy rate is the entropy of the next state given the current state once the process has reached its steady-state distribution.\nErgodic Process: For an ergodic process, the entropy rate can be calculated by considering a long sequence from a single sample path, as it will converge to the same value as the average over many sample paths.\n\nCompression\nOptimal compressor §\nSlepian worlf coding §\nCompression with Side Information at both compressor and decompressor §\nArithmetic coding §\nOptimal compressors for a class of sources §\nRedundancy §\nInformation Projection §\nSanoc theorem §\nEnergy per bit §\nNormalized bit §\nLossy compression §\nScalar quantization §\nDistortion problems §\nConvexity §\nConvex set is defined as the subset S of some vector space which is convex if x,y∈S⟹λx+αy∈S for all 0≤λ≤1.\nConvex functions: f:S→R is:\n\nConvex if f(α1​x+α2​y)≤α1​f(x)+α2​f(y) for all x,y∈S and 0≤α≤1.\nStrictly Convex if f(α1​x+α2​y)&lt;α1​f(x)+α2​f(y) for all x,y∈S and 0≤α≤1.\nStrictly concave if −f is strictly convex.\n\nJensen’s inequality §\nIf f is convex, then f(E[X])≤E[f(X)] for any random variable X.\nEssentially, the left side is the average input and right side the average output. This theorem says that for a convex functions the average input is always less than or equal to the average output.\nThink of a graph which is a simple y=x line. The average input is the average of the x-axis and the average output is the average of the y-axis. As of right now, moving the line up or down will not change the average outputs scale.\nNow as you bend it, the average input will be the same, but the average output will be pushed up. When we bend it more (higher curvature), the differences will increases.\nThere is a very funny example of this:\n\n&gt; Man walks into the bar. \n&gt; Says to the bartender, “One average drink for the average height and weight man” \n&gt; Everyonelaughing.jpeg \n&gt; The bartender says “You are a little fat” \n&gt; Bro doesn’t know why, doesn’t realize weight is related to volume. Volume is roughly the third power of height. So if you are taller, you are expected to be way more heavier \n&gt; Realizes the joke and laughs “I am not fat, I am just convex”\n\nReferences §\n\nJensen’s inequality\n"},"notes/jpdf":{"title":"Joint Probability Density Function (JPDF) for Random Matrices","links":[],"tags":[],"content":"The Joint Probability Density Function (JPDF) is a fundamental concept in the study of random matrices, particularly in understanding the statistical properties of their eigenvalues. It describes how likely it is to find the eigenvalues of a random matrix within a particular range of values, providing insights into the distribution and correlation of these eigenvalues.\nDefinition §\nFor a random matrix, the JPDF of its eigenvalues λ1​,λ2​,…,λn​ is a function P(λ1​,λ2​,…,λn​) that gives the probability density of finding the matrix with its eigenvalues in the infinitesimally small intervals around λ1​,λ2​,…,λn​ simultaneously.\nImportance §\n\n\nSpectral Statistics: JPDF is crucial for understanding the spectral statistics of random matrices, i.e., the statistical properties of their eigenvalues. It serves as the basis for deriving other important statistical measures, such as the level spacing distribution and the spectral density.\n\n\nUniversality: Studies of JPDF have shown that certain statistical properties of eigenvalues are universal, meaning they do not depend on the specific details of the matrix distribution but only on its symmetry class (e.g., GOE, GUE, GSE).\n\n\nApplications: The analysis of JPDF and derived statistics has applications in various fields, including physics (quantum chaos, nuclear physics), mathematics (number theory), and complex systems (network theory, econophysics).\n\n\nCalculation §\nThe explicit form of the JPDF depends on the ensemble of random matrices considered (e.g., GOE, GUE, GSE) and is typically derived using methods from statistical physics and probability theory. For example, for the Gaussian Unitary Ensemble (GUE), the JPDF of eigenvalues can be expressed in a form that involves a Vandermonde determinant, indicating repulsion between eigenvalues:\nP(λ1​,λ2​,…,λn​)∝i&lt;j∏​(λi​−λj​)2exp(−i=1∑n​2σ2λi2​​),\nwhere σ2 is the variance of the entries of the matrix.\nConclusion §\nThe study of JPDF in random matrices reveals profound insights into the behavior of eigenvalues, reflecting both mathematical structures and potential physical phenomena. The patterns observed, such as eigenvalue repulsion and universal statistical behaviors, underscore the deep connections between random matrix theory and other areas of mathematics and physics."},"notes/layerexplain":{"title":"Part 1 Layer","links":[],"tags":["seed"],"content":"Layer code §\nBasically represent the equation: Wx+b where W is the weight matrix and b is the bias.\nclass Layer:\n    \n    def __init__(M,N):\n        self.weight = self.random.random((M,N))\n        self.bias = np.zeros(M)\n    \n    def __forward(x):\n        return np.inner(W,x) + self.bias\n        \n\nThe forward pass is done by the inner product of the weight and the input plus the bias. The problem is the weight initialization is from the standard normal.\nThis is a problem because:\n\nVanishing Gradient Problem\n\nIf the weights are too small the gradient goes to zero and there is no learning\n\n\nExploding Gradient Problem\n\nIf the weights are too large the gradient explodes and the outputs make no sense\n\n\n\nDetailed understanding of the problem §\nIf we initalize the weights from a standard normal distribution. We can see that the values across layers basically go to zero as we go deeper.\n\n\n\nMathematical proof of problem: §\nf′(sk​)≈1,\nVar[zi]=Var[x]i′=0∏i−1​Var[W′i′],\nWe write Var[W′i′] for the shared scalar variance of all weights at layer i′. Then for a network with d layers,\nVar[∂si∂Cost​]=Var[∂sd∂Cost​]i′=i∏d​ni′+1​Var[W′i′],\nVar[∂wi∂Cost​]=i′=0∏i−1​ni′​Var[W′i′]i′=i∏d−1​ni′+1​Var[W′i′]×Var[x]Var[∂sd∂Cost​]\nAs you see the problem, each layers variance is being multiplied. This is the reason, if we see the graph flattens out as the variance increases all the time.\nTo fix this, we set the variance of the gradients to be same across layers.\nFrom a forward-propagation point of view, to keep information flowing we would like that\n∀(i,i′),Var[zi]=Var[zi′].\nFrom a back-propagation point of view we would similarly like to have\n∀(i,i′),Var[∂si∂Cost​]=Var[∂si′∂Cost​].\nKaiming initialization §\nIn popular terms, we use kaiming and xavier initialization. Kaiming paper takes into account the activation function, whereas Xavier initialization does not. The explanation is described here\nSummary:\n\nLet us assume that, w is symetric across 0 and bias is zero.\nReLU only cares about the positive part\nSo, we care only half of the distribution cause it is symetric therefore we can calculate  E(xl2​)=21​Var(yl−1​)\nWe also know that Var(yl​)=nl​Var(wl​xi​)=nl​(E[wl2​]E[xl2​]−E[wl​]2E[xl​]2) , we set the weights to be 0 mean so the right side is useless. We also know that E(wl2​)=Var(wl​) as mean is zero. Final equation is nl​Var(wl​)E[xl2​]\nLets combine previous two points, we get Var(yl​)=nl​Var(wl​)21​Var(yl−1​)\nFor these to not scale up or down, we need Var(yl​)=Var(yl−1​)\nTherefore, we need Var(wl​)=nl​2​\n\nKaiming Normalization is setting the variance to be nl​2​ §\nModified Code for layer §\nclass Layer:\n    \n    def __init__(self,M,N, bias = None):\n        self.weight = self.__kaiming_init(M,N)\n        self.bias = bias if bias is not None else np.zeros(M)\n    \n    def __forward(self,x):\n        output = np.inner(self.weight,x)\n        if self.bias is not None:\n            return  output + self.bias\n        return temp\n    \n    def __kaiming_init(self,M,N):\n        std_dev = np.sqrt(2.0 / M)\n        return np.random.normal(0, std_dev, size=(M, N))\n     \n\nThis explanation assumes that we are using vanilla neural networks with no normalization. The value of initialization techniques gets lost when we use batch normalization or layer normalization.\nReferences: §\n\nUnderstanding the difficulty of training deep feedforward neural networks\nPierre Ouannes - Initialization\nKaiming He et al. - Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n"},"notes/linearalg":{"title":"Linear Algebra study","links":["tags/some2","notes/quaterion","notes/jpdf","notes/wigersurmise"],"tags":["seed","some2"],"content":"Linear Algebra study §\nIn this webpage, i will be writing notes about my linear algebra study.\nIntroduction of Random Matrix Theory §\nRandom matrix theory is a branch of mathematical physics that studies the statistical properties of matrices. The theory is motivated by the observation that the eigenvalues of large random matrices often exhibit universal statistical behavior. This behavior is independent of the specific details of the matrix, such as its size, shape, or the distribution of its entries. Instead, the statistical properties of the eigenvalues depend only on the symmetry class of the matrix, which is determined by its symmetry properties.\nBooks §\nThe book i am using is Introduction of Random Matrices by Giacomo Livan, Marcel Novaes, Pierpaolo Vivo\nThere are couple of other books. But those are hard to understand. The include:\n\n\nA First Course in Random Matrix Theory\nfor Physicists, Engineers and Data Scientists by Potters and Bouchard \n\nSpecial thanks to @Cider for recommending this\n\n\n\nAn Introduction to Random Matrices\n\n\nYoutube Videos §\n\n\nRandom Matrices in Unexpected Places: Atomic Nuclei, Chaotic Billiards, Riemann Zeta#some2\n\n\nRandom Matrices: Theory and Practice - Lecture 1\n\n\nDig Deep §\nDefinitions §\nComplex matrix §\nAll real numbers are usually written as A+Bi, the same way matrix can be written as A+Bi where A and B are matrices.\nExample:\n[618−2i​2+3i4i​]=[618​20​]+[0−2​34​]\nHermitian matrix §\nA matrix is Hermitian if it is equal to its own conjugate transpose. In other words, a matrix A is Hermitian if A=AH. Proof here\nAlso remember that λ1​≥λ2​≥⋯≥λn​\nExample:\n[21+i​1−i3​]=&gt;[21+i​1−i3​]=&gt;[21+i​1−i3​]\nGaussian Orthogonal Ensemble (GOE) §\n\nSymmetry: Real symmetric matrices. This means that the matrix is equal to its transpose (A=AT), and all elements are real numbers.\nKey Property: The matrices are invariant under orthogonal transformations. If O is an orthogonal matrix (OOT=I), then OAOT is also in the GOE if A is.\nApplications: Models real symmetric systems with time-reversal symmetry, such as certain Hamiltonians in physics.\n\nGaussian Unitary Ensemble (GUE) §\n\nSymmetry: Complex Hermitian matrices. This means the matrix is equal to its conjugate transpose (A=A†), with complex entries.\nKey Property: The matrices are invariant under unitary transformations. If U is a unitary matrix (UU†=I), then UAU† is also in the GUE if A is.\nApplications: Used to model systems without time-reversal symmetry, which is common in quantum mechanics for systems experiencing a magnetic field.\n\nGaussian Symplectic Ensemble (GSE) §\n\nSymmetry: Self-dual quaternion matrices, which can be thought of as a generalization involving quaternions, a type of hypercomplex number.\nKey Property: The matrices are invariant under symplectic transformations. This ensemble captures the properties of systems with a specific kind of time-reversal symmetry and spin-orbit interaction.\nApplications: Models systems that have both time-reversal symmetry and strong spin-orbit interaction effects, which are less common but relevant in certain areas of condensed matter physics.\n\nKey Differences §\n\nMathematical Structure: The primary difference lies in the mathematical structure of the matrices (real vs. complex vs. quaternion) and their symmetry properties (orthogonal vs. unitary vs. symplectic).\nPhysical Applications: Each ensemble is suited to model different physical systems based on their symmetry properties—GOE for real symmetric systems with time-reversal symmetry, GUE for complex Hermitian systems without time-reversal symmetry, and GSE for systems with peculiar time-reversal symmetry and spin interactions.\nStatistical Properties: The level spacing distributions (the distribution of distances between adjacent eigenvalues) differ among the three, reflecting the underlying symmetries and interactions in the modeled physical systems.\n\nLearn about quaternions:  Quaterions and its applications\nStudy §\nWe have a way to generate random standard normal matrix which is symmetric. The code is as follows:\nimport numpy as np\n\ndef random_standard_normal_matrix(m,n):\n    A = np.random.normal(0, 1, size=(m, n))\n    return  np.tril(A) + np.triu(A.T, 1)\n\nTo get the eigenvalues of the matrix, we can use the following code:\nfrom numpy.linalg import eig,eigvals\n\neigvals(A)\n\nSince, these are random matrices, we can generate multiple matrices and then calculate all the stats of these matrices. This is refered to as an ensemble of matrices.\nGenerating the eigenvalues of 100000 matrices of size 3x3:\neigen = []\nfor i in range(100000):\n    eigen += list(eigvals(random_standard_normal_matrix(3,3)))\n\nPlotting the distribution of the standard normal matrix, we get this.\n\n\n\nNow, you can see something intresting here. We have N(μ,σ2) distribution for this new distributions of eigen values.\nThis is called the Wigner’s semi-circle law. The eigenvalues of a random matrix are distributed in a semi-circle. This is a very important property of the eigenvalues of a random matrix.\nAnother important part is how do we compare two ensembles of matrices. This is done by calculating the graph above. But there is a problem. That is for 3×3 matrix. If we increase the size of the matrix 6×6, the graph will look different for the same distribution with the same mean and variance we get a wider graph\n\n\n\nTo normalize this: we run the following code:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import gaussian_kde\n\n\n\n# Use Gaussian KDE to estimate the density of the scaled eigenvalues\ndensity_A = gaussian_kde(np.real(eigen1))\ndensity_B = gaussian_kde(np.real(eigen2))\n\n# Plot the densities\nx = np.linspace(min(np.real(eigen1).min(), np.real(eigen2).min()), \n                max(np.real(eigen1).max(), np.real(eigen2).max()), 1000)\nplt.plot(x, density_A(x), label=&#039;Matrix A&#039;)\nplt.plot(x, density_B(x), label=&#039;Matrix B&#039;)\nplt.legend()\nplt.xlabel(&#039;Scaled Eigenvalue&#039;)\nplt.ylabel(&#039;Density&#039;)\nplt.title(&#039;Comparison of Scaled Eigenvalue Densities&#039;)\nplt.show()\n\nBefore Normalization\n\n\n\nAfter Normalization\n\n\n\nLearn, the JPDF better here: Joint Probability Density Function (JPDF) for Random Matrices\nThere is an another intresting conclusion\nWigner’s Surmise §\nProbability of sampling two eigenvalues ’very close’ to each other (s → 0) is very small. This delta for any two eigenvalues can be calculated by\n2s​e−4s2​\nVisual representation of this is as follows:\n\n\n\ntake a more detailed view Wigner’s Surmise\nThe important (generic) feature is that the eigen vectors are not independent: their jpdf does not in general\nfactorize. The most striking incarnation of this property is the so-called level repulsion (as in Wigner’s\nsurmise): the eigenvalues of random matrices generically repel each other, while independent variables do\nnot have this property. This is a very important property of the eigenvalues of a random matrix.\nFor the standard normal R3 we get these correlation values:\n​1−0.59881399−0.62568476​−0.59881.0.43375837​−0.62560.433758371.​​\nWhich shows these are correlated."},"notes/lossfunc":{"title":"Loss Functions in detail","links":[],"tags":["sapling"],"content":"Cross-Entropy §\nCross-entropy measures the average number of bits needed to encode data from one distribution (the true distribution) using the wrong distribution (the model distribution). It’s used frequently in machine learning to quantify the difference between the true distribution of the data and the predictions made by a model. The definition of cross-entropy between two distribution p and q is given by:\nH(p,q)=−∑x​p(x)logq(x)\nwhere p is the true distribution and q is the model distribution. If we were sending messages, p represents the true distribution of messages and q represents the distribution we use to encode the messages, then cross-entropy gives the average number of bits required to encode messages from the true distribution using the code optimized for the estimated distribution.\nKL Divergence §\nKL divergence measures the inefficiency of assuming that the distribution is q when the true distribution is p. It’s a measure of how one probability distribution diverges from a second, expected probability distribution. KL divergence can be seen as the number of extra bits required to encode samples from p using a code optimized for\nq rather than the optimal code. The formula for KL divergence is:\nDKL​(p∣∣q)=∑x​p(x)logq(x)p(x)​\nKL divergence can also be interpreted in the context of information gain or the number of bits saved when the true distribution p is used instead of the estimated distribution q, relative to the inefficiency introduced by using q. However, it’s not strictly the number of bits saved by knowing the distribution in the sense of switching coding schemes, but rather the difference in the expected number of bits required to represent the data under two different distributions.\nSo, while your descriptions are conceptually on track, it’s important to be precise about what these terms mean and how they are used, especially in contexts like machine learning and information theory."},"notes/mathterms":{"title":"Math terms i have no idea about","links":[],"tags":["sapling"],"content":"In this page, we will be discussing some math terms that are used in machine learning and deep learning that i have no idea about.\nLipschitz continuity §\nGiven two metric spaces (X,dX​) and (Y,dY​), a function f:X→Y is said to be Lipschitz continuous if there exists a constant K≥0 such that for all x1​,x2​∈X.\nAny such K is called a Lipschitz constant for the function f. The smallest constant K for which the inequality holds is called the Lipschitz constant of f.\nThink of a double cone, where the slope of the cone is the Lipschitz constant. The function is Lipschitz continuous if the slope of the cone is bounded.\nKrylov subspace §\nGiven a matrix A∈Rn×n and a vector b∈Rn, the Krylov subspace of order m is defined as:\nKm​(A,b)=span{b,Ab,A2b,…,Am−1b}\nPolyak-Lojasiewicz inequality §\nPolyak-Lojasiewicz inequality is a mathematical inequality that is used to prove the convergence of optimization algorithms. It states that the difference between the function value and the minimum value of the function is bounded by the gradient of the function.\nf(x)−f(x∗)≤2β1​∣∣∇f(x)∣∣2\nwhere f(x) is the function value at x, f(x∗) is the minimum value of the function, ∇f(x) is the gradient of the function at x, and β is a constant that depends on the function.\nThis can also translate to if the magnitude of the gradient of the loss function is greater than constant times the loss function, we have exponential convergence of gradients.\nWe care about a more simpler version of this inequality, the minimum value of the function is hard to guess, so we forget about the f(x∗) term. We first prove this simpler form of the inequality and then we will see how it can be used to prove the convergence of optimization algorithms.\nGeneralization Error Bound §\nRademacher Complexity §"},"notes/optimizers":{"title":"Gradient Descent and optimizers","links":["notes/conditionnumber"],"tags":["sapling"],"content":"Gradient Descent §\nIf wee initialize the parameters at some w0​∈Rd, the gradient descent algorithm updates the parameters as follows:\nwt+1​=wt​−η∇L(wt​)\nwhere η is the learning rate and ∇L(wt​) is the gradient of the loss function L at wt​.\nIt decreases the value of the loss at each iteration, as long as the learning rate is small enough and the value of the gradient is nonzero. Eventually, this should result in gradient descent coming close to a point where the gradient is zero.\nWe can prove that gradient descent works under the assumption that the second derivative of the objective is bounded. Suppose that for some constant μ&gt;0, for all x in the space and for any vector v in Rd,\n∣vT∇2L(x)v∣≤μ∥v∥2.\nHere, ∇2L(x) denotes the matrix of partial derivatives of the loss function L. This is equivalent to the condition that ∥∇2L(x)∥2​≤μ\nStarting from this condition, let’s look at how the objective changes over time as we run gradient descent with a fixed step size. From Taylor’s theorem, there exists a ξ such that\nL(wt+1​)=L(wt​−η∇L(wt​))\n=L(ut​)−η(∇f(ut​))T∇L(ut​)+21​η2(∇L(ut​))T∇2L(ξt​)(∇L(ut​))\n≤L(ut​)−η∥∇L(ut​)∥2+2η2L​∥∇L(ut​)∥2=f(ut​)−η(1−2ηL​)∥∇f(ut​)∥2.\nIf we choose our step size η to be small enough that 1&gt;ηL, then,\nL(ut+1​)≤L(ut​)−2η​∥∇L(ut​)∥2\n2η​∥∇L(ut​)∥2≤L(ut​)−L(ut+1​)\nThe objective is guaranteed to decrease at each iteration.\nNow, if we sum this up across T iterations of gradient descent, we get\n2η​t=0∑T−1​∥∇L(ut​)∥2≤t=0∑T−1​(L(ut​)−L(ut+1​))=L(u0​)−f(uT​)≤L(u0​)−L∗\nwhere L∗ is the global minimum value of the loss function L. From here, we can get\nt∈{0,...,T}min​∥∇L(ut​)∥2≤T1​t=0∑T−1​∥∇L(ut​)∥2≤ηT2(L(u0​)−L∗)​.\nThis means that the smallest gradient we observe after T iterations is getting smaller proportional to 1/T. So gradient descent converges (as long as we look at the smallest observed gradient).\nWe have a metric to measure the rate of this convergence. It is called the condition number (kf​) of the function. The condition number is the ratio of the largest eigenvalue of the Hessian matrix to the smallest eigenvalue of the kernel function. If the condition number is large, the function is ill-conditioned and the convergence of gradient descent is slow.\nkf​=λmin​Kλmax​H​\nIf we take the step size(η) to the inverse of the maximum curvature of the hessian matrix. We have an exponential convergence of the gradient descent algorithm.\nL(wt​)≤(1−kf​1​)TL(w0​)\nA condition number which is big would mean the loss barely changes each step and a small condition number would mean the loss changes a lot each step.\nProof of this is pretty complicated, take a look here\nStochastic Gradient Descent (SGD) §\nMini-Batch Gradient Descent §\nAdaptive Learning Rates §\nAdaGrad §\nRMSProp §\nAdam §"},"notes/quaterion":{"title":"Quaternion","links":[],"tags":[],"content":"Quaternion §\nA quaternion is a type of hypercomplex number that extends complex numbers. It is typically represented as:\nq=a+bi+cj+dk\nwhere:\n\na, b, c, and d are real numbers,\ni, j, and k are the fundamental quaternion units.\n\nQuaternions have properties that make them useful for representing rotations and orientations in three-dimensional space. They avoid the singularity and ambiguity problems of Euler angles and are more compact than rotation matrices.\nWATCH THIS: https://eater.net/quaternions\nOperations with Quaternions §\nQuaternions support addition, subtraction, multiplication, and division, but multiplication is not commutative:\nij=k,ji=−k,jk=i,kj=−i,ki=j,ik=−j,i2=j2=k2=ijk=−1\nQuaternion Conjugate §\nThe conjugate of a quaternion q=a+bi+cj+dk is defined as:\nq∗=a−bi−cj−dk\nNorm of a Quaternion §\nThe norm of a quaternion is given by:\n∥q∥=a2+b2+c2+d2​\nQuaternion Inverse §\nThe inverse of a quaternion q is defined as:\nq−1=∥q∥2q∗​\nQuaternion Matrix §\nA quaternion matrix is a matrix where each element is a quaternion. These matrices can represent complex transformations in higher-dimensional spaces and are used in various fields, including theoretical physics, computer graphics, and robotics.\nApplications §\nQuaternion matrices are particularly useful for:\n\nRotating points in three-dimensional space,\nInterpolating orientations (slerp),\nSimulating rigid body dynamics.\n\nTheir ability to represent rotations without suffering from gimbal lock makes them invaluable in 3D computer graphics and aerospace engineering."},"notes/quotes_random":{"title":"Random quotes","links":[],"tags":[],"content":"Language, as the domain of the intelligible, is presented as an intrinsically computational process of syntactic structuring and semantic ascent"},"notes/tangentkernel":{"title":"Tangent Kernels","links":["notes/mathterms","notes/conditionnumber"],"tags":["sapling"],"content":"A tangent kernel for a function F(w)=y, where w∈Rm and y∈Rn. A tanget kernel is a matrix K∈Rn×n is:\nK(w)=∇F(w)⋅∇F(w)T\nEssential, it is gradient of the function F at w multiplied with its transpose.\nOne important conclusion is if we can get the minimum eigen-value of the tangent kernel is greater than μ, then we satisfy the μ-strong Polyak-Lojasiewicz inequality\nProof: §\nLet us assume λmin​(K(w))≥μM​ on B\nL(w)=21​∣∣F(w)−y∣∣2\nLet us assume that our loss function L(w) expressed as the squared norm of the difference between a function F(w) and some target y.\nLet us consider the gradient of the loss function( derivative of the loss function with respect to the weights)\n21​∣∣∇L(w)∣∣2=21​∣∣(F(w)−y)T∇F(w)∣∣2=21​(F(w)−y)T∇F(w)∇F(w)T(F(w)−y)\nWe know that:\nK=∇F(w)∇F(w)T\nSo, we can rewrite the above equation as:\n21​∣∣∇L(w)∣∣2=21​(F(w)−y)TK(F(w)−y)=K⋅21​∣∣(F(w)−y)∣∣2=K⋅L(w)\nTherefore the gradiant of the loss function is the tangent kernel multiplied by the loss function.\nIf we use the initial conditions, we can rewrite the above equation as:\n21​∣∣∇L(w)∣∣2≥λmin​(K(w))⋅L(w)\nIf we see the kernel function, the smallest eigenvalue of the kernel function is λmin​(K). If λmin​(K)≥μM​, then we have the μM​-strong Polyak-Lojasiewicz inequality. This translates to high convergence of the optimization algorithm.\nThis is a very important result as it shows that the minimum eigenvalue of the tangent kernel is a key factor in the convergence of the optimization algorithm. If the minimum eigenvalue of the tangent kernel is greater than a certain threshold, then the optimization algorithm will converge exponentially fast. This is a very useful result in practice, as it allows us to analyze the convergence properties of optimization algorithms and design algorithms that converge faster.\nNow a natural question is how we measure the convergence. Read more about in condition number"},"notes/transformerdeep":{"title":"Transformers from first principles","links":["notes/layerexplain","notes/ff","notes/batchnorm","notes/lossfunc","notes/inftheory","notes/distribution","notes/compression"],"tags":["evergreen"],"content":"In this page, we will implement Attention is all you need, from scratch. This is a very simple implementation and is not optimized for speed. The goal is to understand the architecture of the transformer. If you are a basic understanding of linear algrebra and python, you should be able to follow along.\nSubcomponents of the transformer which need to be coded out in seperate sections:\nArchitecture §\n\nLayer\n\nKaiming Initialization\n\n\nFeed Forward network\nAttention Block\nSelf Attention Block\nMulti Head Attention Block\nEmbedding Layer\nPositional Encoding\nLayer normalization\nBatch normalization\nActivation functions\nMasking\nDropout\nEncoder\nDecoder\nTransformer\nTokenizers\n\nTraining §\n\n[[notes/optimizers||Optimizers]]\n\nWhy adam takes into account the root mean and expected value of the divergece ?\n\n\nLoss functions\nRegularization\nWeight decay\nLearning rate schedulers\n\nDecoding §\n\nBeam search\nGreedy search\nTemperature sampling\nEvaluation metrics\n\n\nI am speed. - Lightning McQueen\n\nDeep Learning go brrrrrrr §\n\nGradient Checkpointing\nMixed Precision Training\nDistributed Training\n\nModel Parallelism\nData Parallelism\n\n\nQuantization\nPruning\nKnowledge Distillation\nTransfer Learning\nFine Tuning\nPretraining\n\nThere are several topics which are required to have a basic intution about why is it learning. Here are some topics, I will be covering which go behind why does this blob of code learn:\n\nInformation Theory\nDistributions\nCompression\n"},"notes/weaponizesae":{"title":"Weaponizing SAEs","links":[],"tags":["evergreen"],"content":"\nWelcome to the age of turbo maya-induced schizoprenia\n-sponsored by golden-gate adtech\n\nCouple of months ago you might have noticed Golden Gate claude. Researchers effectively found that there appears to be a some features which are highly abstract, multilingual and multimodal.\nThese features can be effectively used to steer AI agents to be behave in a certain manner. The did find a semantic relationship between the frequency of concepts and the dictionary size needed to resolve features for them. This post is a way to apply the skills and better understand the future of these systems.\nLet begin with a simple question:\nAre we devouring our own tail? §\nImagine, if you will, a more conniving, unethical version of myself. This digital doppelganger isn’t content with using AI for the greater good. Oh no, they have far more sinister plans in mind.\nArmed with this new knowledge, our nefarious alter ego sets out to manipulate these AI systems, weaving an intricate web of deception. Their goal? To bend the very fabric of artificial intelligence to their will, creating a puppet show where both the AI and unsuspecting users dance to their malevolent tune. He doesn’t recognize the irony of his actions or the slow descent to his madness of his own making.\n\n    \n\nBefore we imagine how he would do it , let us understand the basics.\nBackground §\nSteer Vectors §\nSAE §\nLinear representation §\nSuperposition §\nMonosiminticity §\nDictionary Learning §\nExperimentation §\nSteering vectors §\nhttps://vgel.me/posts/representation-engineering/\nSAE §\nMemes §\n\n    \n\n&gt; Ripped the meme from @bycloyd \nPersonal take §\nThis post is a way to think about the implications of this finding and what is means for the future of these systems. Personally, I believe these tools can change the way we interact with these systems but givens it power, it can be used to do anything. As someone who works in E-commerce materialism does pervade my judgement.\nWarning: This post is a case study of one possible way how LLMs can be monetized by advertising"},"notes/wigersurmise":{"title":"Wigner’s surmise","links":[],"tags":["seed"],"content":"Consider a 2 x 2 GOE matrix  Hs​=(x1​x3​​x3​x2​​)  with x1​,x2​∼N(0,1) and x3​∼N(0,1/2). What is the pdf ( \\rho(s) ) of the spacing s=λ2​−λ1​ between its two eigenvalues (λ2​&gt;λ1​)?\nThe two eigenvalues are random variables, given in terms of the entries by the roots of the characteristic polynomial\nλ2−Tr(Hs​)λ+det(Hs​),\nTherefore λ1,2​=(x1​+x2​±(x1​−x2​)2+4x32​​)/2 and s=(x1​−x2​)2+4x32​​.\nBy definition, we have\nρ(s)=∫−∞∞​∫−∞∞​∫−∞∞​2π​2π​π​e−21​x12​e−21​x22​e−x32​​δ(s−(x1​−x2​)2+4x32​​)dx1​dx2​dx3​.\nChanging variables as\n⎩⎨⎧​x1​−x2​=rcosθ2x3​=rsinθx1​+x2​=ψ​⇒⎩⎨⎧​x1​=2rcosθ+ψ​x2​=2ψ−rcosθ​x3​=2rsinθ​​\nand computing the corresponding Jacobian\nJ=det​∂r∂x1​​∂r∂x2​​∂r∂x3​​​∂θ∂x1​​∂θ∂x2​​∂θ∂x3​​​∂ψ∂x1​​∂ψ∂x2​​∂ψ∂x3​​​​=det​cos2θ​−cos2θ​sin2θ​​−2rsin2θ​​2rsin2θ​​2rcos2θ​​​21​21​0​​=−4r​,\nOne obtains\np(s)=8π3/21​∫0∞​drrδ(s−r)∫02π​dθ∫−∞∞​dψe−21​[(2rcosθ+ψ​)2+(2ψ−rcosθ​)2+2r2sin2θ​]=8π3/24πs​​∫02π​dθe−2s2​[2cos2θ​+2sin2θ​]=2s​e−4s2​"},"tags/dreams":{"title":"Dreams","links":[],"tags":[],"content":""},"tags/evergreen":{"title":"Evergreen","links":[],"tags":[],"content":""},"tags/sapling":{"title":"Sapling","links":[],"tags":[],"content":""},"tags/seed":{"title":"Seed","links":[],"tags":[],"content":""}}